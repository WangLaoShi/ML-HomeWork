#  自适应学习率算法

## AdaGrad
AdaGrad是Adaptive Gradient的缩写，适应方法是每个参数都有自己的学习率，学习率是和每个参数的梯度相关的，而且是累积的；算出一个参数的梯度之后，会去计算累积的平方梯度，如果这个参数已经被更新了多次，二阶动量大，那么学习率就小；反之更新少的参数就会有一个比较大一些的学习率去更新
对不同的参数，使用不同的学习率，对于经常更新的参数和偶尔更新的参数采用不同的学习率，利用所有梯度值的平方和作为更新频率度量。
缺点：更新频度单调，可能提前结束训练过程


##   RMSprop
RMSprop是AdaGrad的一种变体，就是将动量累积和当前时刻的梯度做了一个加权求和（滑动平均），这么做的目的是为了让之前的梯度对当前影响变小

##  AdaDelta
Adadelta和RMSprop一样就是做了一个加权求和（滑动平均），但是不一样的是用近段的一个时间窗口内的动量累积，而不是历史动量累积，这就更能避免AdaGrad的第一个问题。
这里不计入全部的历史梯度，不是把所有更新频率都算进去，所以避免单调的学习率，只关注过去一段时间的窗口的梯度下降。

利用了指数移动平均值，来确定过去一段时间的梯度下降。


## Adam
Adam是Momentum + RMSprop；
在算完梯度后，更新一阶矩估计s和二阶矩估计r，s作为梯度动量累积（Momentum），r作为学习率的分母（RMSprop）；最后的公式如果没有r的就是Momentum了，如果把s换成g就是RMSprop。

### Momentum特点

特点：相比于SGD，考虑了历史累计梯度。
优点：旨在加速学习，特别是高曲率、小但一致的梯度，或是带噪声的梯度。动量算法累积了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。


# 岭回归（Ridge）

## Ridge原理
岭回归也是一种用于回归的线性模型，因此它的预测公式与普通最小二乘法相同。但在岭回归中，对系数（w）的选择不仅要在训练数据上得到好的预测结果，而且还要拟合附加约束。
同时希望系数尽量小。换句话说，w 的所有元素都应接近于 0。
直观上来看， 这意味着每个特征对输出的影响应尽可能小（即斜率很小），同时仍给出很好的预测结果。这种约束是所谓正则化（regularization）的一个例子。正则化是指对模型做显式约束，以避免过拟合。岭回归用到的这种被称为 L2 正则化。

通常岭回归方程的R平方值会稍低于普通回归分析，但回归系数的显著性往往明显高于普通回归，在存在共线性问题和病态数据偏多的研究中有较大的实用价值。

