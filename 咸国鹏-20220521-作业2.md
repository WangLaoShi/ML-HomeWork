# lr 调优
<br>

## 1. Full gradient & Backtracking Line Search
- 基于Armijo-Goldstein condition 的一种线搜索方法。确定在给定方向移动，在一个较大的估计步长进行迭代回溯，组件缩小步长，获得一个合适的步长，即学习率。**步长优化算法**
- 应用情况：适用于梯度下降，当然也可以用于其他上下文算法，例如：牛顿迭代和海森矩阵等
- 给定下降方向，迭代更新步长；
![](https://pic1.zhimg.com/80/v2-a0e74ae40580cda9d197229cfdb0a328_720w.jpg)

```text
目标函数：f
参变量：x
超参数：\alpha, \beta
方向: \Delta x
步长: t
```
![](https://pic3.zhimg.com/80/v2-4bef433d091899ce5a8ad8d76cca3796_720w.jpg)
如果图1中等式成立，那么经过推导可知，\alpha 要满足下式：
![](https://www.zhihu.com/equation?tex=%EF%BC%88%5Cfrac%7Bf%28x%2Bt%5CDelta+x%29+-+f%28x%29%7D%7Bt%5CDelta+x%7D%29+%2F++%5Cnabla+f%28x%29+%3E+%5Calpha)
当 t ->0 时，上式是满足的，而回溯直线搜索是由大到小变化步长 `t` , `t` 较大时，左式可看做在X点的斜率，斜率很可能要比  \alpha，要大，上诉不等式就不成立了，而越接近最优点，斜率越小，当斜率大于学习率时，说明斜率还在较大的位置，步长太小，需要迭代考虑更小的  \alpha。


## 2. SGD & AdaDelta
当前函数对参数的梯度：

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zY1k3bVpCa2dldDRtTGlhOWlhMGNpYkxFWDBNYWljaFhQZmFlUGtDcHFMdUY4aFY4OVVKSHRhRUYzd0tHTXdBbExKUGlhSlp1MTluQURZbDJDQmoxY2lhWW1pYlEvNjQw?x-oss-process=image/format,png)

- SGD：随机梯度下降
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zY1k3bVpCa2dldDRtTGlhOWlhMGNpYkxFWDBNYWljaFhQZmFLY1RDSFkxdEZLWE1WMjZxNVhubjNKQzdpY2U4eGhyaWM5RnNlMWliWGdQVFh1WkpLUXNSNzM2WmcvNjQw?x-oss-process=image/format,png)

- 缺点：下降速度慢或者两边震荡，达不到全局最优，可能停留在某个局部最优点

- AdaGrad
- 对不同的参数，使用不同的学习率，对于经常更新的参数和偶尔更新的参数采用不同的学习率，利用所有梯度值的平方和作为更新频率度量。
- 二阶动量
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zY1k3bVpCa2dldDRtTGlhOWlhMGNpYkxFWDBNYWljaFhQZmEzdmpXVGtqSzJ3MEZpYTFCV3ZWQlU1NlF4dXJWME9BQjRya2libGhGQzRSMkZFOHlJNGE1d0dPdy82NDA?x-oss-process=image/format,png)

- 下降梯度
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zY1k3bVpCa2dldDRtTGlhOWlhMGNpYkxFWDBNYWljaFhQZmFvMXR6U2ljR0trUjIwQVlkYjhtYlpGYnhXREV5ZWdNMDlvaWJzWHVqQmNtTGdYTHNWcFRmOWVxQS82NDA?x-oss-process=image/format,png)


学习率变为![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9zY1k3bVpCa2dldDRtTGlhOWlhMGNpYkxFWDBNYWljaFhQZmFEanM5RDNKd1JNMXhaS0tsdHhjTEZ0aWJOZk9KWWdhNkhTY2lhamh2aWNUWHNTSG93bHNRYnNXUXcvNjQw?x-oss-process=image/format,png),这就根据更新频率的不同来设置不同的学习率，避免对某些经常更新的参数仍然大量更新。

- 缺点：更新频度单调，可能提前结束训练过程。

- AdaDelta
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9uVENlWUd3b3FZM2x1Smc3YWZEN3hTUGpIYmxRNmpBU05YdUhLNktLb3EwOVBRSjI0WGFUSWljdnFrYmdWS3Fvd2JZRTgwcGphWGZyT2M3MFBDc25DRFEvNjQw?x-oss-process=image/format,png)
- 比起上一个算法，这里不计入全部的历史梯度，不是把所有更新频率都算进去，所以避免单调的学习率，只关注过去一段时间的窗口的梯度下降。
- 利用了指数移动平均值，来确定过去一段时间的梯度下降。

- Adam
- 一阶动量 + 二阶动量
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9uVENlWUd3b3FZM2x1Smc3YWZEN3hTUGpIYmxRNmpBUzVxcTVXTUZINUQ2UjFmUFlQbnZSb21uUmlhREZKaHpzdXU4T0ltWDV6dzJxWDlqWkpIWUNQRXcvNjQw?x-oss-process=image/format,png)
![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9uVENlWUd3b3FZM2x1Smc3YWZEN3hTUGpIYmxRNmpBU1pIZEhaR1ZxM0lOaWJSZjJUOG1INTRVS25DT0hQaWNVY2ttZ3dsQ3Myek5laWNub2ljTmdWM1ppY2lhUS82NDA?x-oss-process=image/format,png)
- 结合一阶二阶动量，对参数进行优化