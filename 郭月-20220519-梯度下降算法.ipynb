{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking line search 回溯线性搜索\n",
    "线搜索的目的是在搜索方向上找到是目标函数$f(x)$最小的点。然而，精确找到最小点比较耗时，由于搜索方向本来就是近似，所以用较小的代价找到最小点的近似就可以了。Backtracking line search（BLS）就是这样一种线性搜索算法\n",
    "\n",
    "**BLS算法的思想是：在搜索方向上，先设置一个初始步长$\\alpha _0$,如果步长太大，则缩减步长，直到合适为止**\n",
    "1. 如何判断当前步长是否合适：\n",
    "\n",
    "<center> $ f(x+\\alpha p)<=f(x)+\\alpha cm $ </center>\n",
    "<center>$ m=p^T \\nabla f(x)  $ </center>\n",
    "其中，P是当前搜寻方向，$\\alpha$是步长，c是控制参数，需要根据情况人工核定。\n",
    "\n",
    "从上式可以看出，当前点的斜率越小，$ f(x+\\alpha p)-f(x)$的要求越小，步长就越小。对于一般的凸问题，搜寻点越接近最优点，原函数的斜率就七月小，因此步长越小，这也是符合直觉的。\n",
    "\n",
    "2. 如何缩减步长\n",
    "搜索步长的缩减通过$\\tau $参数来控制，主要通过人工核定，即 $\\alpha _j=\\tau \\alpha_(j-1)\n",
    "\n",
    "总结BLS的算法流程：\n",
    "\n",
    "(1)设置初始步长 $\\alpha _0$\n",
    "\n",
    "(2)判断$ f(x+\\alpha p)<=f(x)+\\alpha cm $ 是否满足，如果满足，停止；否则(3)\n",
    "\n",
    "(3)$\\alpha _j=\\tau \\alpha_(j-1)$，重复2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.2944 0.7056000000000012\n",
      "0.6502809600000009 0.05531904000000035\n",
      "0.05098202726400033 0.004337012736000018\n",
      "0.003996990937497611 0.00034002179850240664\n",
      "0.00031336408949982016 2.665770900258648e-05\n",
      "2.456774461678329e-05 2.089964385803191e-06\n",
      "6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure,plot, show, xlabel, ylabel, legend\n",
    "def f(x):\n",
    "        \"The function we want to minimize\"\n",
    "        return (x-3)**2\n",
    "def f_grad(x):\n",
    "        \"gradient of function f\"\n",
    "        return 2*(x-3)\n",
    "x = 0\n",
    "y = f(x)\n",
    "err = 1.0\n",
    "maxIter = 300\n",
    "curve = [y]\n",
    "it = 0\n",
    "step = 0.1\n",
    "# Backtracking line search 的python实现\n",
    "x = 0\n",
    "y = f(x)\n",
    "err = 1.0\n",
    "alpha = 0.25\n",
    "beta = 0.8\n",
    "curve2 = [y]\n",
    "it = 0\n",
    " \n",
    "while err > 1e-4 and it < maxIter:\n",
    "    it += 1\n",
    "    gradient = f_grad(x)\n",
    "    step = 1.0\n",
    "    while f(x - step * gradient) > y - alpha * step * gradient**2:\n",
    "        step *= beta\n",
    "    x = x - step * gradient\n",
    "    new_y = f(x)\n",
    "    err = y - new_y\n",
    "    y = new_y\n",
    "    print (err,y)\n",
    "    curve2.append(y)\n",
    " \n",
    "# print 'iterations: ', it\n",
    "print(it)\n",
    "plot(curve2, 'bo-')\n",
    "legend(['gradient descent I used', 'backtracking line search'])\n",
    "show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自适应的learning rate\n",
    " ### AdaGrad算法\n",
    "\n",
    "Adagrad优化算法被称为自适应学习率优化算法，之前我们讲的随机梯度下降对所有的参数都使用的固定的学习率进行参数更新，但是不同的参数梯度可能不一样，所以需要不同的学习率才能比较好的进行训练，但是这个事情又不能很好地被人为操作，所以 AdaGrad 便能够帮助我们做这件事。\n",
    "\n",
    "<center>$ n_t=n_(t-1)+g_t^2$</center>\n",
    "<center>$ \\Delta \\theta _t=-\\frac{\\eta}{\\sqrt{n_t+\\epsilon}}*g_t $</center>\n",
    "\n",
    "其中$\\eta$是学习率，$g_t$是梯度\n",
    "**AdaGrad 的核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新** \n",
    "\n",
    "https://github.com/ivallesp/awesome-optimizers/blob/master/Report.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, cost_f, lr=0.001, x=None, y=None):\n",
    "        super().__init__(cost_f=cost_f, lr=lr, x=x, y=y)\n",
    "        self.sumsq_dx = 0\n",
    "        self.sumsq_dy = 0\n",
    "        \n",
    "    def step(self, lr = None):\n",
    "        epsilon = 1e-8\n",
    "        if not lr:\n",
    "            lr = self.lr\n",
    "        # derivative\n",
    "        f = self.cost_f.eval(self.x, self.y)\n",
    "        dx = self.cost_f.df_dx(self.x, self.y)\n",
    "        dy = self.cost_f.df_dy(self.x, self.y)\n",
    "        self.sumsq_dx += dx**2\n",
    "        self.sumsq_dy += dy**2\n",
    "        self.x = self.x - (lr/(np.sqrt(epsilon + self.sumsq_dx)))*dx\n",
    "        self.y = self.y - (lr/(np.sqrt(epsilon + self.sumsq_dy)))*dy\n",
    "        \n",
    "        return [self.x, self.y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaDelta 算法\n",
    "\n",
    "AdaDelta算法是对AdaGrad算法的改进。相比于AdaGard算法，AdaDelta算法有两大优势：(1) 解决了AdaGard算法学习率不断降低的问题，(2)动态确定学习率， 不需要提前设置学习率这一超参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaDelta(Optimizer):\n",
    "    def __init__(self, cost_f, lr=0.001, decay_rate=0.9, x=None, y=None):\n",
    "        super().__init__(cost_f=cost_f, lr=lr, x=x, y=y, decay_rate=decay_rate)\n",
    "        self.decay_x = 0\n",
    "        self.decay_y = 0\n",
    "        self.decay_dx = 1\n",
    "        self.decay_dy = 1\n",
    "        \n",
    "    def step(self, lr=None, decay_rate=None):\n",
    "        epsilon = 1e-8\n",
    "        if not lr:\n",
    "            lr = self.lr\n",
    "        if not decay_rate:\n",
    "            decay_rate = self.decay_rate\n",
    "        # derivative\n",
    "        f = self.cost_f.eval(self.x, self.y)\n",
    "        dx = self.cost_f.df_dx(self.x, self.y)\n",
    "        dy = self.cost_f.df_dy(self.x, self.y)\n",
    "        # Update decays\n",
    "        self.decay_x = decay_rate * (self.decay_x) + (1-decay_rate)*dx**2\n",
    "        self.decay_y = decay_rate * (self.decay_y) + (1-decay_rate)*dy**2\n",
    "        \n",
    "        update_x = dx*((np.sqrt(epsilon + self.decay_dx))/(np.sqrt(epsilon + self.decay_x)))\n",
    "        update_y = dy*((np.sqrt(epsilon + self.decay_dy))/(np.sqrt(epsilon + self.decay_y)))\n",
    "        \n",
    "        self.x = self.x - (update_x)*lr\n",
    "        self.y = self.y - (update_y)*lr\n",
    "        \n",
    "        # Update decays d\n",
    "        self.decay_dx = decay_rate * (self.decay_dx) + (1-decay_rate)*update_x**2\n",
    "        self.decay_dy = decay_rate * (self.decay_dy) + (1-decay_rate)*update_y**2\n",
    "        \n",
    "        return [self.x, self.y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop 算法\n",
    "特点：\n",
    "\n",
    "其实RMSprop依然依赖于全局学习率\n",
    "RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间\n",
    "适合处理非平稳目标 - 对于RNN效果很好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, cost_f, lr=0.001, decay_rate=0.9, x=None, y=None):\n",
    "        super().__init__(cost_f=cost_f, lr=lr, x=x, y=y, decay_rate=decay_rate)\n",
    "        self.ms_x = 0\n",
    "        self.ms_y = 0\n",
    "        \n",
    "    def step(self, lr=None, decay_rate=None):\n",
    "        epsilon = 1e-8\n",
    "        if not lr:\n",
    "            lr = self.lr\n",
    "        if not decay_rate:\n",
    "            decay_rate = self.decay_rate\n",
    "        # derivative\n",
    "        f = self.cost_f.eval(self.x, self.y)\n",
    "        dx = self.cost_f.df_dx(self.x, self.y)\n",
    "        dy = self.cost_f.df_dy(self.x, self.y)\n",
    "        self.ms_x = self.decay_rate * (self.ms_x) + (1-self.decay_rate)*dx**2\n",
    "        self.ms_y = self.decay_rate * (self.ms_y) + (1-self.decay_rate)*dy**2\n",
    "        self.x = self.x - (lr/(epsilon + np.sqrt(self.ms_x)))*dx\n",
    "        self.y = self.y - (lr/(epsilon + np.sqrt(self.ms_y)))*dy\n",
    "        \n",
    "        return [self.x, self.y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam算法\n",
    "Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。\n",
    "\n",
    "特点：\n",
    "结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点\n",
    "对内存需求较小\n",
    "为不同的参数计算不同的自适应学习率\n",
    "也适用于大多非凸优化 - 适用于大数据集和高维空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaDelta(Optimizer):\n",
    "    def __init__(self, cost_f, lr=0.001, decay_rate=0.9, x=None, y=None):\n",
    "        super().__init__(cost_f=cost_f, lr=lr, x=x, y=y, decay_rate=decay_rate)\n",
    "        self.decay_x = 0\n",
    "        self.decay_y = 0\n",
    "        self.decay_dx = 1\n",
    "        self.decay_dy = 1\n",
    "        \n",
    "    def step(self, lr=None, decay_rate=None):\n",
    "        epsilon = 1e-8\n",
    "        if not lr:\n",
    "            lr = self.lr\n",
    "        if not decay_rate:\n",
    "            decay_rate = self.decay_rate\n",
    "        # derivative\n",
    "        f = self.cost_f.eval(self.x, self.y)\n",
    "        dx = self.cost_f.df_dx(self.x, self.y)\n",
    "        dy = self.cost_f.df_dy(self.x, self.y)\n",
    "        # Update decays\n",
    "        self.decay_x = decay_rate * (self.decay_x) + (1-decay_rate)*dx**2\n",
    "        self.decay_y = decay_rate * (self.decay_y) + (1-decay_rate)*dy**2\n",
    "        \n",
    "        update_x = dx*((np.sqrt(epsilon + self.decay_dx))/(np.sqrt(epsilon + self.decay_x)))\n",
    "        update_y = dy*((np.sqrt(epsilon + self.decay_dy))/(np.sqrt(epsilon + self.decay_y)))\n",
    "        \n",
    "        self.x = self.x - (update_x)*lr\n",
    "        self.y = self.y - (update_y)*lr\n",
    "        \n",
    "        # Update decays d\n",
    "        self.decay_dx = decay_rate * (self.decay_dx) + (1-decay_rate)*update_x**2\n",
    "        self.decay_dy = decay_rate * (self.decay_dy) + (1-decay_rate)*update_y**2\n",
    "        \n",
    "        return [self.x, self.y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![梯度下降算法的效果比较](https://pic1.zhimg.com/5d5166a3d3712e7c03af74b1ccacbeac_r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge 的作用\n",
    "\n",
    "Ridge回归是一种专用于共线性数据分析的有偏估计回归方法。Ridge的作用是保持所有变量，例如使用所有变量来建立模型，同时赋予它们重要度，从而提高模型的性能。当数据集中变量数量较少时，岭是一个很好的选择，因此需要所有这些变量来解释得到的“洞察力”和预测目标结果。\n",
    "\n",
    "由于Ridge保持了所有变量的完整性，并且lasso在分配变量的重要度方面做得更好，因此，结合Ridge和Lasso的最佳特性，组合出了“弹性网络”，作为开发算法。弹性网络是更理想的选择。\n",
    "\n",
    "在进行机器学习时，有更多方法来进行特征选择，基本思想通常保持不变：显示特征的重要度，然后根据获得的“重要度”消除变量。这里的重要度是一个非常主观的术语，因为它不是一个度量，而是度量和图形的集合，可以用来检查最重要的特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
