{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7050f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "import seaborn as sns\n",
    "from ipywidgets import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69849b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('paper', font_scale=2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    '''original function to minimize'''\n",
    "    return x1 ** 2 +  x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):\n",
    "    '''the gradient dfdx1 and dfdx2'''\n",
    "    dfdx1 = 0.2 * x1\n",
    "    dfdx2 = 4 * x2\n",
    "    return dfdx1, dfdx2\n",
    "\n",
    "def train_2d(trainer, lr):\n",
    "    \"\"\"Train a 2d object function with a customized trainer\"\"\"\n",
    "    x1, x2 = -5, -2\n",
    "    s_x1, s_x2 = 0, 0\n",
    "    res = [(x1, x2)]\n",
    "    for i in range(50):\n",
    "        x1, x2, s_x1, s_x2, lr = trainer(x1, x2, s_x1, s_x2, lr)\n",
    "        res.append((x1, x2))\n",
    "    return res\n",
    "\n",
    "def plot_2d(res, figsize=(10, 6), title=None):\n",
    "    x1_, x2_ = zip(*res)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot([0], [0], 'r*', ms=15)\n",
    "    plt.text(0.0, 0.25, 'minimum', color='w')\n",
    "    plt.plot(x1_[0], x2_[0], 'ro', ms=10)\n",
    "    plt.text(x1_[0]+0.1, x2_[0]+0.2, 'start', color='w')\n",
    "    plt.plot(x1_, x2_, '-o', color='#ff7f0e')\n",
    " \n",
    "    plt.plot(x1_[-1], x2_[-1], 'wo')\n",
    "    plt.text(x1_[-1], x2_[-1]-0.25, 'end', color='w')\n",
    "    x1 = np.linspace(-5.5, 3, 50)\n",
    "    x2 = np.linspace(min(-3.0, min(x2_) - 1), max(3.0, max(x2_) + 1), 100)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    plt.contourf(x1, x2, f_2d(x1, x2), cmap='rainbow')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a58bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaGrad 算法\n",
    "核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，\n",
    "而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabdc8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d78ca99494f486793af20870bf4ed10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='lr', max=4.0, step=0.01), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 4, 0.01),\n",
    "          continuous_update=False)\n",
    "def visualize_adagrad(lr=0.1):\n",
    "    '''lr: learning rate'''\n",
    "    def adagrad_2d(x1, x2, s1, s2, lr):\n",
    "        g1, g2 = f_grad(x1, x2)\n",
    "        eps = 1e-6\n",
    "        s1 += g1 ** 2\n",
    "        s2 += g2 ** 2\n",
    "        x1 -= lr / math.sqrt(s1 + eps) * g1\n",
    "        x2 -= lr / math.sqrt(s2 + eps) * g2\n",
    "        return x1, x2, s1, s2, lr\n",
    " \n",
    "    res = train_2d(adagrad_2d, lr)\n",
    "    plot_2d(res, title='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaDelta算法\n",
    "Adagrad因为要考虑参数使用的频繁性等，所以会累加所有历史梯度，造成动量的持续累积，\n",
    "导致学习率的变化过快。AdaDelta不累加所有历史梯度，而只关注过去一段时间的梯度。\n",
    "通过对过去一段时间的梯度做平方版本的指数加权平均，来避免动量的持续积累问题。\n",
    "由于使用了指数加权平均，所以可以进一步解决震荡问题，并进一步加快收敛速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84d83c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fecc17d24047599b118e51b3ec0c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='lr', max=1.0, step=0.001), FloatSlider(value=0.9, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 1, 0.001), \n",
    "          gamma=(0, 0.99, 0.001),\n",
    "          continuous_update=False)\n",
    "def visualize_rmsprop(lr=0.1, gamma=0.9):\n",
    "    '''lr: learning rate, \n",
    "       gamma: momentum'''  \n",
    "    def rmsprop_2d(x1, x2, s1, s2, lr):\n",
    "        eps = 1e-6\n",
    "        g1, g2 = f_grad(x1, x2)\n",
    "        s1 = gamma * s1 + (1 - gamma) * g1 ** 2\n",
    "        s2 = gamma * s2 + (1 - gamma) * g2 ** 2\n",
    "        x1 -= lr / math.sqrt(s1 + eps) * g1\n",
    "        x2 -= lr / math.sqrt(s2 + eps) * g2\n",
    "        return x1, x2, s1, s2, lr\n",
    "\n",
    "    res = train_2d(rmsprop_2d, lr)\n",
    "    plot_2d(res, title='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8340f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam算法\n",
    "Adam是结合了Adagrad和RMSProp的优点，通过计算梯度的一阶矩估计和二阶矩估计为不同参数设计独立的自适应学习率，\n",
    "避免了单一学习率的问题，也避免了训练过程中学习率不变的问题。因此能解决稀疏梯度和噪声问题，调参也相对简单。\n",
    "但Adam中L2正则化并不像在SGD中那么有效，许多最优的效果还是Momentum细调出来的\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7374259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "accd7c595c2f496d904d4c201fdd6956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='lr', max=1.0, step=0.001), FloatSlider(value=0.9, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 1, 0.001), \n",
    "          beta1=(0, 0.999, 0.001),\n",
    "          beta2=(0, 0.999, 0.001),\n",
    "          continuous_update=False)\n",
    "def visualize_adam(lr=0.1, beta1=0.9, beta2=0.999):\n",
    "    '''lr: learning rate\n",
    "    beta1: parameter for E(g)\n",
    "    beta2: parameter for E(g^2)\n",
    "    '''  \n",
    "    def Deltax(m, n, g, t):\n",
    "        eps = 1.0E-6\n",
    "        m = beta1 * m + (1 - beta1) * g\n",
    "        n = beta2 * n + (1 - beta2) * g*g\n",
    "        m_hat = m / (1 - beta1**t)\n",
    "        n_hat = n / (1 - beta2**t)\n",
    "        dx = lr * m_hat / (math.sqrt(n_hat) + eps)\n",
    "        return m, n, dx\n",
    " \n",
    "    def adam_2d(x1, x2, m1, n1, m2, n2, lr, t):\n",
    "        '''m1, m2: E(g1), E(g2)\n",
    "           n1, n2: E(g1^2), E(g2^2) where E() is expectation\n",
    "           lr: learning rate\n",
    "           t: time step'''\n",
    "        eps = 1e-6\n",
    "        g1, g2 = f_grad(x1, x2)\n",
    "        m1, n1, dx1 = Deltax(m1, n1, g1, t)\n",
    "        m2, n2, dx2 = Deltax(m2, n2, g2, t)       \n",
    "        x1 -= dx1\n",
    "        x2 -= dx2\n",
    "        return x1, x2, m1, n1, m2, n2, lr\n",
    " \n",
    "    def train_adam(trainer, lr):\n",
    "        \"\"\"Train a 2d object function with a customized trainer\"\"\"\n",
    "        x1, x2 = -5, -2\n",
    "        m1, n1, m2, n2 = 0, 0, 0, 0\n",
    "        res = [(x1, x2)]\n",
    "        for i in range(30):\n",
    "            x1, x2, m1, n1, m2, n2, lr = trainer(x1, x2, m1, n1, m2, n2, lr, i+1)\n",
    "            res.append((x1, x2))\n",
    "        return res\n",
    " \n",
    "    res = train_adam(adam_2d, lr)\n",
    "    plot_2d(res, title='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd42c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
