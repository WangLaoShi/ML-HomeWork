{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f62b1f3",
   "metadata": {},
   "source": [
    "# 20220514 作业一"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6e717",
   "metadata": {},
   "source": [
    "Backtracking line search\n",
    "\n",
    "\n",
    "给定起始位置x和搜索方向p，行搜索的任务是确定步长a充分降低目标函数f，即查找a这减少了f(x + ap)相对于f.\n",
    "\n",
    "\n",
    "回溯线搜索从大的估计值开始并迭代缩小它。收缩继续进行，直到找到一个值，该值足够小，可以提供目标函数的减少，该减少与基于局部函数梯度的预期减少充分匹配delta(f(x))\n",
    "定义函数的局部斜率a沿搜索方向p, m = delta(fx)与p的点积\n",
    "\n",
    "基于选定的控制参数c(在0-1之间)，Armijo-Goldstein 条件检验是否从当前位置进行逐步移动x到修改后的位置 x + ap \n",
    "\n",
    "回溯行搜索算法表示：\n",
    "tao在0-1，c在0-1\n",
    "设置 t = -cm，迭代计数器j = 0\n",
    "在满足条件之前 f(x) -f( x+ a(j)p)>= a(j)*t . 递增j  a(j )= tao* a(j-1)\n",
    "返回a(j)\n",
    "\n",
    "\n",
    "在实践中使用回溯线搜索的功能最小化:\n",
    "在实践中，上述算法通常被迭代以产生序列x(n) 收敛到最小值，对于梯度下降，p(n) = -delta(f(x(n)))\n",
    "选择初始起点x0并设置迭代计数器n=0.\n",
    "在满足某些停止条件之前，选择下降方向pn,增加n，并将位置更新为x(n+1) = x(n) + a(x(n),p(n))p(n)\n",
    "返回x(n)作为最小化位置和f(x(n))作为函数最小值。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af1a51f2",
   "metadata": {},
   "source": [
    "AdaGrad\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/29920135\n",
    "起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。\n",
    "在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179a073",
   "metadata": {},
   "source": [
    "AdaDelta\n",
    "\n",
    "https://www.zhihu.com/question/483367614\n",
    "\n",
    "AdaDelta算法与RMSProp算法一样，都属于对AdaGrad算法的改进。相比于AdaGard算法，AdaDelta算法有两大优势：\n",
    "(1) 像RMSProp算法一样解决了AdaGard算法学习率不断降低的问题，\n",
    "(2)动态确定学习率， 不需要提前设置学习率这一超参数。\n",
    "以外，AdaDelta算法还修改了分子一项，这一做法的动机是一阶方法（如梯度下降法）满足  ，而更加准确的二阶方法（牛顿法）满足  。\n",
    "二阶方法中，参数的变化量正比于参数，随着参数的变大，梯度也以相应比例变大。\n",
    "AdaGrad算法并没有对一阶算法的这一缺陷作出改进，\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b6d98",
   "metadata": {},
   "source": [
    "ADAM\n",
    "\n",
    "https://www.zhihu.com/question/323747423\n",
    "\n",
    "RMSprop非常高效，但没发表的适应性学习率方法。有趣的是，使用这个方法的人论文中都引用自Geoff Hinton的Coursera课程的第六课的第29页PPT。他修改了Adagrad方法，让方法不那么激进。具体说来，就是它使用了一个梯度平方的滑动平均：\n",
    "\n",
    "cache = decay_rate * cache + (1 - decay_rate) * dx**2 \n",
    "x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "\n",
    "decay_rate是一个超参数，常用0.9 0.99 0.999\n",
    "因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但其更新不会让学习率单调变小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52854690",
   "metadata": {},
   "source": [
    "# 20220514 作业二"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516bf8f",
   "metadata": {},
   "source": [
    "正则项：让预测值对于单一自变量X的敏感程度降低，具体降低的程度取决于正则化力度，方式。\n",
    "\n",
    "Ridge Regression），这样模型的目标函数除了经验风险最小化，额外引入了结构风险最小化的部分\n",
    "Ridge回归的正则化的梯度会随着临近最低点而减小，在接近最小值的时候其梯度也会变小，所以不会真的变成0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
