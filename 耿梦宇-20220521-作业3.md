
# 1.loss波动厉害的原因
在进行梯度下降寻找极值时，导致loss震荡波动很厉害的原因有输入的**数据存在问题、训练的batch_size太小、loss函数设计的不合理、激活函数选用不合理、学习率太大、优化算法不合理、正则化选择问题**一共可能的7个原因。
## 1.输入的数据问题
数据集太少
* 输入的数据集没有经过标准化，可能出现异常点导致loss的震荡，或者数据的量纲不一致

## 2.训练的batch_size太小
batch的选择首先决定的是下降方向。如果batch_size选择的过小，同时类别有很多，那么每次迭代的方向可能是不好的局部最优点，会导致训练速度很慢，训练不容易收敛。

## 3.loss函数设计的不合理
在深度学习中，不同的loss对应的任务是不一样的。有比较通用的如L1,L2函数，但有一些任务需要对应的loss函数会有更好的效果。选择了错误或不好的损失函数也会造成学习过程中loss震荡比较明显，或不容易收敛。  

常用模型对应的loss函数如下:
* logloss(对数损失函数) 
> * logloss常用来作为逻辑回归模型的损失函数。

原理是逻辑回归模型的核心在于假设样本满足的是伯努利0-1分布，求解theta的方法是通过对似然函数转成对数似然函数，求对数似然函数的极值。而不是求似然函数的极值。从损失函数的角度来看，逻辑回归的损失函数就变成了log损失函数。


* hinge loss（合页损失函数）
> * 合页损失函数常用来作为SVM等经典二分类问题的损失函数。

hinge loss的基本思想是分类正确，且函数距离大于1的损失的样本为0，分类错误的样本损失为样本到超平面的距离。要求是不仅要正确分类，而且确信度足够高损失才是0，对学习有更高的要求。

原理是SVM等二分类模型的策略是间隔最大化，把样本分成两类。每一个样本只有两种可能，要么是正确分类的，要么是错误分类的。对于正确分类的样本去、并且间隔大于1，我们认为没有损失，损失为0。对于错误分类的样本，我们希望能通过迭代，调整超平面，来让错误样本离超平面的距离尽可能的大，实现间隔最大化。

* exp-loss(指数损失函数)
> exp-loss常作为集成方法Boosting中Adaboost算法的损失函数。

Adaboost算法采用加权多数表决的方法，基本原理就是调整样本权重的方式对样本的分布进行调整。通过提高错误分类样本的权重，降低正确分类样本的权重，通过求损失函数最小以达到减小错误样本影响的作用。
对Adaboost的推到（这里略）发现，Adaboost的目标式子就是指数损失。

* cross-entropy loss(交叉熵损失函数)
> cross-entropy loss常用来作为softmax的损失函数。

交叉熵刻画的是通过预测值的概率分布q来表达正确答案的概率分布p的难度。交叉熵越小，表示正确值和预测值的概率分布越接近，预测的越准。

softmax通常用于神经网络的末端。相对比于hardmax只得到一个确定的值，softmax更倾向于有不同的输出结果，将结果以概率的形式输出，表示每个概率的可能性。[关于softmax函数的参考资料](https://zhuanlan.zhihu.com/p/105722023)

通过softmax把原始输出值转化成概率的时候，我们需要知道真实的概率分布和预测的概率分布之间的距离。目标就是使这个距离越来越小。

* quadratic loss（平方误差损失函数）
> quadratic loss常用于线性回归

最小二乘法

* absolution loss (绝对值损失函数)
> 用预测值于真实值之间距离的绝对值表示损失

* 0-1 loss (0-1损失函数)

> 0-1 loss直接对应分类判断错误的个数

不考虑预测值和真实值之间的偏差，分类正确损失为0，分类错误损失为1。

## 4.激活函数选用不合理
大多数情况下，都几乎使用RELU作为全局激活函数，尽可能少的使用sigmode激活函数。
[激活函数参考资料](https://baijiahao.baidu.com/s?id=1582399059360085084&wfr=spider&for=pc)

* sigmode的激活范围太小，当不在这个激活范围中的时候容易造成梯度弥散、消失。

> Y轴范围0-1，x轴在-3到3之间的梯度非常高，但在其他区域形状很平坦。这意味着在[-3，3]这个范围内，x的少量变化也将导致y值的大幅度变化。一旦x值不在[-3，3]内，梯度就变得很小，接近于零，而网络就得不到真正的学习。因此，函数本质上试图将y值推向极值。 当我们尝试将值分类到特定的类时，使用Sigmoid函数非常理想。

* RELU函数可以激动所有x>0的神经元，也意味着不会同时激活所有的神经元，如果输入值是负的，ReLU函数会转换为0，而神经元不被激活。这意味着，在一段时间内，只有少量的神经元被激活，神经网络的这种稀疏性使其变得高效且易于计算。

## 5.学习率太大
学习率太大会导致一个step前进的路程太长，就会迟迟收敛不到最优点，出现来回震荡的情况。

## 6.优化算法不合理
一般来说使用Adam作为优化器的默认参数。但有些情况下经过仔细调整的SGD算法性能可能更好
 * Adam算法整合了自适应学习率于动量项，算法用梯度构造了两个向量m和v，前者为动量项，后者累积了梯度的平方和（二阶动量），用于构造自适应学习率。
* SGD的改进算法：
SGD with Momentum：SGD-M认为梯度下降过程可以加入惯性，在SGD基础上引入了一阶动量。动量项就是考虑了前一步对后一步的影响，参数更新是上一时刻的动量项与本次梯度值的加权平均值。因此动量项累积了之前迭代时的梯度值，使得本次迭代时沿着之前的惯性方向向前走。
SGD with Nesterov Acceleration：是在SGD、SGD-M的基础上的进一步改进，改进点在于在当前位置不计算当前位
置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向。然后用下一个点的梯度方
向，与历史累积动量相结合，计算走一步之后时刻的累积动量。相比于SGD-M，考虑了后两步的影响。

[优化算法参考资料]（https://blog.csdn.net/qq_38384924/article/details/120808518）

## 7.正则化
正则化项是对权值参数的惩罚项，目的是减轻模型的复杂度，防止过拟合，增强泛化能力。有可能正则化项的选择有问题，依旧导致了过拟合，造成loss的震荡

# 2.梯度下降方法
## 1.全量梯度下降
批量梯度下降在更新参数是使用所有的样本来进行更新。计算训练集所有样本误差，对其求和再取平均值作为目标函数。缺点是下降的速度很慢，而且对无法处理超出内存容量限制的数据集。FG通常用line search来确定每一次下降的方向和步长。

### line_search:线搜索。
迭代的求得某个函数的最值的方法. 对于每次迭代, 线搜索会计算得到搜索的方向pk以及沿这个方向移动的步长αk.也就是目标函数f(x)在规定的一个方向上移动所形成的单变量优化问题。
### Exact line search精确直线搜索
也叫做射线搜索。
指求$\alpha_k$ 使目标函数f沿方向 $d_k$达到 极小.
基本思想是首先确定包含问题最优解的搜索区间，然后采用某种插值或分割技术缩小区间，进行搜索求解。有两种方法。
* 进退法——确定搜索区间并保证具有 近似单峰性质 的数值算法。由于不少精确线搜索算法都是针对单峰函数建立起来的，但实际中很多函数都不是单峰的

```python
function [a,b]=Brackeing(f,x0,h,t)
flag=0;
#给出初始值x1和x2，h为步长
x2=x0;
x1=x0+h;
#比较每一步的走前后的函数值
if f(x2)==f(x1)
    while f(x2)==f(x1)
       x1=(x1+x2)/2; 
    end
    if f(x2)>f(x1)
        x3=2*x1-x2;
        flag=1;
    end
end

if f(x2)<f(x1)
    x1=x1+x2;
    x2=x1-x2;
    x1=x1-x2;
    h=-h;
end

if flag==0
    h=t*h;
    x3=x1+h;
    while f(x3)<f(x2)
        x2=x1;
        x1=x3;
        h=t*h;
        x3=x1+h;
    end
    if f(x3)==f(x1)
        xm=(X1+x3)/2;
        while f(xm)==f(x1)
            x3=xm;
            xm=(x1+x3)/2;
        end
        if f(xm)>f(x1)
            x3=xm;
        end
        if f(xm)<f(x1)
            x2=x1;
            x1=xm;
        end
    end 
end
a=min([x1 x2 x3]);
b=max([x1 x2 x3]);
end
```
* 黄金分割法。
  选取区间以及精度，进行每一个点的试探计算，决定每一步的步长。

```python
function [x,miny,k]=Golden(f,a,b,esp)
l=a+0.382*(b-a);
r=a+0.618*(b-a);
k=1;

while(b-a)>esp
    if f(l)>f(r)
        a=l;
        l=r;
        r=a+0.618*(b-a);
    else
        b=r;
        r=l;
        l=a+0.382*(b-a);
    end
    k=k+1;
end
x=(a+b)/2;
miny=f(x);
end
```
* 基本牛顿法
```python
function [x,minf,k]=Newton(f,x0,eps)
% 其中 f 为符号函数
df=diff(f);
ddf=diff(df);
f=inline(f);
df=inline(df);
ddf=inline(ddf);

k=0;

while abs(df(x0))>eps
    x0=x0-df(x0)/ddf(x0);
    k=k+1;
end

x=x0;
```
> Backtracking line search回溯直线搜索。这是一种非精确直线搜索方法，指选取\alpha_k使目标函数f得到 可以接受的下降量.
```python
x=200*rand(1,2);

y = f(x);

err = 1.0;
alpha = 0.25;
beta = 0.8;
iter = 0;

%% Gradient
while err > 1e-8  
    iter =iter+ 1;

    % Gradient vector
    gradient1 = f_gradx1(x);
    gradient2 = f_gradx2(x);
    Grad=[gradient1,gradient2];

    % Step size
    step = 1;
    while f(x - step * Grad) > f(x) - alpha * step * Grad*Grad.'
        step = step*beta;
    end

    % Update
    step
    x = x - step * Grad;
    new_y = f(x)
    err =  abs(y-new_y)
    y = new_y
    
end
```
## 2.随机梯度下降sgd
随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。

* 最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来**优化波动**(扰动), 但随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向**从当前的局部极小值点跳到另一个更好的局部极小值点**，这样便可能**对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。**

```python
def stochasticGradientDescent(x, y, theta, alpha, m, maxInteration):
     data = []
     for i in range(4):
         data.append(i)
     x_train = x.transpose()
     for i in range(0, maxInteration):
         hypothesis = np.dot(x, theta)
         # 损失函数
         loss = hypothesis - y
         # 选取一个随机数
         index = random.sample(data, 1)
         index1 = index[0]
         # 下降梯度
         gradient = loss[index1] * x[index1]
         # 求导之后得到theta
         theta = theta - alpha * gradient
     return theta
  
  
 def main():
     trainData = np.array([[1, 4, 2], [2, 5, 3], [5, 1, 6], [4, 2, 8]])
     trainLabel = np.array([19, 26, 19, 20])
     print(trainData)
     print(trainLabel)
     m, n = np.shape(trainData)
     theta = np.ones(n)
     print(theta.shape)
     maxInteration = 500
     alpha = 0.01
     
     theta2 = stochasticGradientDescent(trainData, trainLabel, theta, alpha, m, maxInteration)
     print(theta2)
     return
  
  
 if __name__ == "__main__":
     main()

```
# 3.岭回归-Ridge模型——多重共线性问题
岭回归模型的最早引入是为了处理线性回归时样本特征矩阵不可逆的情况。现在也用于在估计中加入偏差，从而得到更好的估计，也可以用来解决多重共线性问题。
* 是一种专用于共线性数据分析的有偏估计回归方法,实质上是一种改良的最小二乘估计法,通过放弃最小二乘法的无偏性,以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法,对病态数据的拟合要强于最小二乘法
* 基本做法是在线性回归的似然函数中加入正则项，用引入少量偏差为代价减少拟合模型的方差，使新拟合模型在测试数据中表现的更好，防止过拟合。

[Ridge模型参考资料](https://cloud.tencent.com/developer/article/1699240)
