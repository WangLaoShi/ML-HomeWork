import pprint

import numpy as np
import sys
sys.path.append('/Volumes/WD_BLACK/å›½é™…äºº/IPS-Teaching-Material/ä¸ƒé¾™ç è®¡åˆ’/4. æœºå™¨å­¦ä¹ è®­ç»ƒè¥/IPS-ML-Teaching/2-çº¿æ€§å›å½’ä»£ç å®ç°/çº¿æ€§å›å½’-ä»£ç å®ç°/utils')
from features import prepare_for_training

# åŸæ–‡åœ°å€ï¼šhttps://www.cnblogs.com/xiugeng/p/12977373.html

class LinearRegression:
    '''
    çº¿æ€§å›å½’å®ç°.åˆ°ç›®å‰ä¸ºæ­¢è¿˜æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ–¹æ³•
    '''

    def __init__(self, data, labels, polynomial_degree=0, sinusoid_degree=0, normalize_data=True):
        """
        1.å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†æ“ä½œ
        2.å…ˆå¾—åˆ°æ‰€æœ‰çš„ç‰¹å¾ä¸ªæ•°
        3.åˆå§‹åŒ–å‚æ•°çŸ©é˜µ
        """

        print("-"*50)
        print("~~~~~~~~åŸå§‹çš„æ•°æ®é›†~~~~~~~~")
        pprint.pprint(data)

        (data_processed,
         features_mean,
         features_deviation) = prepare_for_training(data, polynomial_degree, sinusoid_degree, normalize_data=True)
        print("æ•°æ®é›†çš„å¹³å‡å€¼" + str(features_mean))
        print("æ•°æ®é›†çš„å‡å·®å€¼" + str(features_deviation))
        print("~~~~~~~~å¤„ç†åæ•°æ®é›†~~~~~~~~")
        pprint.pprint(data_processed)
        print("-" * 50)
        self.data = data_processed
        self.labels = labels # æ ‡è®°
        self.features_mean = features_mean # å‡å€¼
        self.features_deviation = features_deviation # æ ‡å‡†å·®
        self.polynomial_degree = polynomial_degree # å¤šé¡¹å¼
        self.sinusoid_degree = sinusoid_degree # sin å€¼
        self.normalize_data = normalize_data # è§„èŒƒåŒ–
        # è·å–å¤šå°‘ä¸ªåˆ—ä½œä¸ºç‰¹å¾é‡
        num_features = self.data.shape[1]# 1 æ˜¯åˆ—ä¸ªæ•°ï¼Œ0 æ˜¯æ ·æœ¬ä¸ªæ•°
        self.theta = np.zeros((num_features, 1))# æ„å»ºÎ¸çŸ©é˜µ

        print("-"*50)
        print("~~~~~~~~~~~~~~~~~~åˆå§‹åŒ–çš„æ—¶å€™å¾—åˆ°äº†ä»€ä¹ˆä¸œè¥¿~~~~~~~~~~~~~~~~~~")
        print("æ ·æœ¬æ•°(è®°å½•çš„è¡Œæ•°)" + str(self.data.shape[0]))
        print("ç‰¹å¾æ•°(è®°å½•çš„åˆ—æ•°)" + str(self.data.shape[1]))
        print("æœ€åŸå§‹çš„ theta å€¼")
        print(self.theta)
        print("-"*50)

    def train(self, alpha, num_iterations=500):
        """
        è®­ç»ƒæ¨¡å—ï¼Œæ‰§è¡Œæ¢¯åº¦ä¸‹é™
        :param alpha: Î±ä¸ºå­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰
        :param num_iterations: è¿­ä»£æ¬¡æ•°
        """
        cost_history = self.gradient_descent(alpha, num_iterations)
        return self.theta, cost_history

    def gradient_descent(self, alpha, num_iterations):
        """
        å®é™…è¿­ä»£æ¨¡å—ï¼Œä¼šè¿­ä»£ num_iterations æ¬¡
        :param alpha: å­¦ä¹ ç‡
        :param num_iterations: è¿­ä»£æ¬¡æ•°
        """
        cost_history = []# ä¿å­˜æŸå¤±å€¼
        for _ in range(num_iterations):
            print("ç¬¬ " + str(_) + " æ¬¡å¾ªç¯~~~~")
            self.gradient_step(alpha)# æ¯æ¬¡è¿­ä»£å‚æ•°æ›´æ–°
            cost_history.append(self.cost_function(self.data, self.labels))
        return cost_history

    # çœ‹æ‡‚è¿™ä¸ªå†çœ‹å‡½æ•°ï¼Œ![0mx7hh](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/0mx7hh.png)
    def gradient_step(self, alpha):
        """
        æ¢¯åº¦ä¸‹é™å‚æ•°æ›´æ–°è®¡ç®—æ–¹æ³•ï¼ˆæ ¸å¿ƒä»£ç ,çŸ©é˜µè¿ç®—ï¼‰
        :param alpha: å­¦ä¹ ç‡
        """
        num_examples = self.data.shape[0] # æ ·æœ¬æ•°
        prediction = LinearRegression.hypothesis(self.data, self.theta) # é¢„æµ‹å€¼
        # æ®‹å·® = é¢„æµ‹å€¼-çœŸå®å€¼
        delta = prediction - self.labels
        theta = self.theta
        # thetaå€¼æ›´æ–°ï¼Œ.Tæ˜¯æ‰§è¡Œè½¬ç½®
        # ![lsUHor](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/lsUHor.png)
        theta = theta - alpha * (1 / num_examples) * (np.dot(delta.T, self.data)).T
        self.theta = theta

    def cost_function(self, data, labels):
        """
        æŸå¤±è®¡ç®—
        :param data: æ•°æ®é›†
        :param labels: çœŸå®å€¼
        :return:
        """
        num_examples = data.shape[0]  # æ ·æœ¬ä¸ªæ•°
        # æ®‹å·® = é¢„æµ‹å€¼-çœŸå®å€¼
        delta = LinearRegression.hypothesis(self.data, self.theta) - labels
        cost = (1 / 2) * np.dot(delta.T, delta) / num_examples
        print("ğŸ‘‡"*25)
        print("å·´æ‹‰å·´æ‹‰å°é­”ä»™~~~~~~~~~~~~~~~å½“å‰æŸå¤±~~~~~~~~~~~~~~~")
        print(cost)
        print('ğŸ‘†ğŸ»'*25)
        return cost[0][0]

    @staticmethod
    def hypothesis(data, theta):
        '''
        å¾—åˆ°é¢„æµ‹å€¼ï¼ˆå‡è®¾å€¼ï¼‰
        :param data:
        :param theta:
        :return:
        '''
        # å¦‚æœå¤„ç†çš„æ˜¯ä¸€ç»´æ•°ç»„ï¼Œåˆ™å¾—åˆ°çš„æ˜¯ä¸¤æ•°ç»„çš„å…§ç§¯ï¼›å¦‚æœå¤„ç†çš„æ˜¯äºŒç»´æ•°ç»„ï¼ˆçŸ©é˜µï¼‰ï¼Œåˆ™å¾—åˆ°çš„æ˜¯çŸ©é˜µç§¯
        print("#"*25)
        print("å˜›å“©å˜›å“©å“„~~~~~~~è®©æˆ‘çœ‹çœ‹æ¯æ¬¡çš„ theta å€¼,å®ƒå°±æ˜¯æˆ‘ä»¬çš„ X çš„å‰é¢çš„å˜é‡,å®ƒæ˜¯é æ¢¯åº¦åœ¨æ”¹å˜")
        print(theta)
        print("#" * 25)
        predictions = np.dot(data, theta)
        return predictions

    def get_cost(self, data, labels):
        '''
        è·å– cost æˆæœ¬
        :param data:
        :param labels:
        :return:
        '''
        # ç»è¿‡å¤„ç†äº†çš„æ•°æ®
        data_processed = prepare_for_training(data,
                                              self.polynomial_degree,
                                              self.sinusoid_degree,
                                              self.normalize_data
                                              )[0]
        # è¿”å›æŸå¤±å€¼
        return self.cost_function(data_processed, labels)

    def predict(self, data):
        """
        ç”¨è®­ç»ƒçš„å‚æ•°æ¨¡å‹ï¼Œä¸é¢„æµ‹å¾—åˆ°å›å½’å€¼ç»“æœ
        """
        data_processed = prepare_for_training(data,
                                              self.polynomial_degree,
                                              self.sinusoid_degree,
                                              self.normalize_data
                                              )[0]

        predictions = LinearRegression.hypothesis(data_processed, self.theta)

        return predictions
