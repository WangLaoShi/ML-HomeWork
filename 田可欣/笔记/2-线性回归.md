# 线性回归

Created Time: May 14, 2022 12:07 AM

上课时间: May 8, 2022

重点: 每个人对代码有自己的理解，没有绝对的正确

Materials: 

- [机器学习线性回归算法.pdf](https://github.com/kk37111754/ML-HomeWork/blob/75aa1e94fe62d94f0c0b54fa463f7cf2c82c957a/%E7%94%B0%E5%8F%AF%E6%AC%A3/%E7%AC%94%E8%AE%B0/files/2.%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95.pdf) 
- [CleanMultiVarLinearRegression.py](https://github.com/kk37111754/ML-HomeWork/blob/75aa1e94fe62d94f0c0b54fa463f7cf2c82c957a/%E7%94%B0%E5%8F%AF%E6%AC%A3/%E7%AC%94%E8%AE%B0/files/2.CleanMultiVarLinearRegression.py) 
- [LinearRegression.py](https://github.com/kk37111754/ML-HomeWork/blob/75aa1e94fe62d94f0c0b54fa463f7cf2c82c957a/%E7%94%B0%E5%8F%AF%E6%AC%A3/%E7%AC%94%E8%AE%B0/files/2.LinearRegression.py) 
- [MultiVariateLinearRegression.py](https://github.com/kk37111754/ML-HomeWork/blob/75aa1e94fe62d94f0c0b54fa463f7cf2c82c957a/%E7%94%B0%E5%8F%AF%E6%AC%A3/%E7%AC%94%E8%AE%B0/files/2.MultiVariateLinearRegression.py) 
- [normalize.py](https://github.com/kk37111754/ML-HomeWork/blob/75aa1e94fe62d94f0c0b54fa463f7cf2c82c957a/%E7%94%B0%E5%8F%AF%E6%AC%A3/%E7%AC%94%E8%AE%B0/files/2.normalize.py) 
- [prepare_for_training.py](https://github.com/kk37111754/ML-HomeWork/blob/75aa1e94fe62d94f0c0b54fa463f7cf2c82c957a/%E7%94%B0%E5%8F%AF%E6%AC%A3/%E7%AC%94%E8%AE%B0/files/2.prepare_for_training.py) 

# 机器学习和数学的区别

| 数学               | 机器学习                               |
| ------------------ | -------------------------------------- |
| 对于误差容忍度较小 | 对于误差有一定的容忍                   |
| 结果相对唯一       | 结果不唯一<br />==只要接受，就是正确== |

# 线性回归

## 超平面

### 拟合结果特征

- 平面上下的点分布均匀
- 平面上下的点到平面的距离相似

### 公式

![超平面公式](https://latex.codecogs.com/svg.image?\begin{aligned}&space;h_{\theta}(x)&space;&=\varepsilon&plus;\theta_{1}&space;x_{1}&plus;\theta_{1}&space;x_{1}&plus;\ldots&plus;\theta_{n}&space;x_{1}&space;\\&space;&=\varepsilon&plus;\sum_{i=1}^{n}&space;\theta_{i}&space;x_{i}&space;\\&space;&=\varepsilon&plus;\theta^{T}&space;x&space;\end{aligned}) 其中，![点积](https://latex.codecogs.com/svg.image?\theta^{T}&space;x=\left(\begin{array}{c}\theta_{1},&space;\\&space;\theta_{2},&space;\\&space;\vdots,&space;\\&space;\theta_{n}\end{array}\right)&space;\cdot\left(x_{1},&space;x_{2},&space;\ldots,&space;x_{n}\right))

- ![\theta](https://latex.codecogs.com/svg.image?\theta)的维度与变量![x](https://latex.codecogs.com/svg.image?x)的个数有关

## 关键词

### 维度

与变量![x](https://latex.codecogs.com/svg.image?x)的个数有关

### Label 结果

结果![y](https://latex.codecogs.com/svg.image?y)的值

### Feature 特征

包含变量![x](https://latex.codecogs.com/svg.image?x)和结果![y](https://latex.codecogs.com/svg.image?y)

### 样本

数据数量

- 训练集 train set：
  - 估计模型
- 验证集 validation set：
  - 确定网络结构或者控制模型复杂程度的参数
- 测试集 test set：
  - 检验最终选择最优的模型的性能如何
- 验证集与测试集的比例相同
- 数据从样本中随机抽取

## 解题过程

### 目的

最终得到一个/组合适的![\theta](https://latex.codecogs.com/svg.image?\theta)值使超平面拟合

### 降维

删减不相关的特征以减少计算难度

### 斜率初始化

- 随机给出一个/组斜率值，为后期计算铺垫
- 通常为0值

### 误差

- 预测值与真实值之间的差值
- 通过更新![\theta](https://latex.codecogs.com/svg.image?\theta)值，使误差减小

### 梯度下降

- 批量梯度下降 BGD
- 随机梯度下降 SGD
- 小批量梯度下降 MSGD
- 结果公式：

  ![梯度下降结果](https://latex.codecogs.com/svg.image?\begin{aligned}&space;\theta_{i}&space;&=\theta_{i}-\alpha&space;\frac{\partial&space;J(\theta)}{\theta_{i}}&space;\\&space;&=\theta_{i}-\alpha&space;\frac{1}{m}&space;\sum_{j=1}^{m}\left(h_{\theta}\left(x^{(j)}\right)-y^{(j)}\right)&space;x_{i}^{(j)}&space;\quad(i=0,1,&space;\ldots,&space;n)&space;\end{aligned})
  
    - ![\alpha](https://latex.codecogs.com/svg.image?\alpha)：学习效率
    - ![误差值](https://latex.codecogs.com/svg.image?h_{\theta}\left(x^{(j)}\right)-y^{(j)})：误差值
      - ![预测值](https://latex.codecogs.com/svg.image?h_{\theta}\left(x^{(j)}\right))：预测值
      - ![真实值](https://latex.codecogs.com/svg.image?y^{(j)})：真实值
    - ![数据](https://latex.codecogs.com/svg.image?x_{i}^{(j)})：数据

### 总结

1. 对原始数据集进行处理
2. 随机假定![\theta](https://latex.codecogs.com/svg.image?\theta)值
3. 进行学习

## 优化方法

- 增加样本数量
- 更新损失函数
- 替换梯度下降
- 优化归一方法
