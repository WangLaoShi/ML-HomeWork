陈静茹-20220514-作业12

# lr decay

在梯度下降过程中，学习率较大时搜索过程容易发生震荡（搜索步长迈太大，局部最优点迈过去了）

如果能够让lr随着迭代周期不断衰减变小，，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法。

# Line search

## 无约束非线性规划

![image-20220515140036729](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220515140036729.png)

f（x）目标函数

x在整个线性空间，没有约束。（如果有约束，x只能在容许域里）

目的：我们想找到一个 ![[公式]](https://www.zhihu.com/equation?tex=x%5E%2A) 使得 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 的值尽可能的小。

## 具体步骤

选取初始点x0

![image-20220515235329692](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220515235329692.png)

（下降方向是所有能使x等高线指更小的方向，负梯度方向是垂直指向等高线里面的方向，下降最快）

选取下降方向p0，沿p0做直线搜索

![[公式]](https://www.zhihu.com/equation?tex=x%5E%7Bk%2B1%7D+%3D+x%5Ek+%2B+s%5Ek+d%5Ek)

且我们希望，也就是说一步步向最优点靠近。

![[公式]](https://www.zhihu.com/equation?tex=f%28x%5E%7Bk%2B1%7D%29%3Cf%28x%5Ek%29)

根据下降方向选取的不同和步长因子 $s^k$ 的选取不同，可以分为

直接法：（不用计算梯度）

- 步长加速法
- 黄金分割法

解析法：

- 最速下降法
- 牛顿法
- 共轭梯度法
- 拟牛顿法

（具体步骤请搜索凸优化，我有点讲不明白）

# 自适应的learning rate

## SGD

为了找到最优参数，我们将参数的梯度（导数）作为了线索。

使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent）

![image-20220516010350836](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516010350836.png)

SGD简单，并且容易实现，但是在解决某些问题时可能没有效率，可能会出现z字形移动的情况。

![image-20220516010555733](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516010555733.png)

## 　Momentum

![image-20220516010655745](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516010655745.png)

新出现了一个变量**v**，对应物理上的速度。上式表示了物体在梯度方向上受力，在这个力的作用下，物体的速度增加。更新路径就像小球在碗中滚动一样。

![image-20220516011035068](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516011035068.png)

和SGD相比，我们发现“之”字形的“程度”减轻了。这是因为虽然*x*轴方向上受到的力非常小，但是一直在同一方向上受力，所以朝同一个方向会有一定的加速。反过来，虽然*y*轴方向上受到的力很大，但是因为交互地受到正方向和反方向的力，它们会互相抵消，所以*y*轴方向上的速度不稳定。因此，和SGD时的情形相比，可以更快地朝*x*轴方向靠近，减弱“之”字形的变动程度。

## AdaGrad

AdaGrad会为参数的每个元素适当地调整学习率，与此同时进行学习（Ada来自英文单词Adaptive）对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

![image-20220516011249828](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516011249828.png)

怎么样去度量历史更新频率呢？那就是迄今为止所有梯度值的[平方和](https://www.zhihu.com/search?q=平方和&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"32230623"})变量**h**保存了以前的所有梯度值的平方和（上式中的**表示对应矩阵元素的乘法）。然后，在更新参数时，通过乘以 
$$
\frac{1}{\sqrt{h}}
$$
，就可以调整学习的尺度。这意味着，参数的元素中变动较大（被大幅更新）的元素的学习率将变小。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。

![image-20220516011848323](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516011848323.png)

## AdaDelta

由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。

![image-20220516013335100](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516013335100.png)





## Adam

Adam是2015年提出的新方法。它的理论有些复杂，直观地讲，就是融合了Momentum和AdaGrad的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。此外，进行超参数的“偏置校正”也是Adam的特征。

![image-20220516012033327](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516012033327.png)

# Ridge

L2正则化，正则化的作用：防止过拟合。**让预测值对于单一自变量X的敏感程度降低，具体降低的程度取决于我们的正则化力度，方式**。

![image-20220516013815756](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516013815756.png)

权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合

为损失函数加上权重的平方范数（L2范数）。这样一来，就可以抑制权重变大。用符号表示的话，如果将权重记为**W**，L2范数的权值衰减就是 ![image-20220516014543408](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516014543408.png)

，然后将这个惩罚加到损失函数上。这里，*λ*是控制正则化强度的超参数。*λ*设置得越大，对大的权重施加的惩罚就越重。

L2范数相当于各个元素的平方和。用数学式表示的话，假设有权重

**W** = (*w*1*, w*2*,* ... *, w**n*)，则 L2范数可用 ![image-20220516014713407](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220516014713407.png)

计算出来。