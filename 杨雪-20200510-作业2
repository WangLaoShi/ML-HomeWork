# ADAGRAD
Adagrad优化算法被称为自适应学习率优化算法，之前我们讲的随机梯度下降对所有的参数都使用的固定的学习率进行参数更新，但是不同的参数梯度可能不一样，所以需要不同的学习率才能比较好的进行训练，但是这个事情又不能很好地被人为操作，所以 Adagrad 便能够帮助我们做这件事
考虑优化问题：

minx∈Rnf(x)=1N∑i=1Nfi(x).
AdaGrad 算法在随机梯度下降法的基础上，通过记录各个分量梯度的累计情况， 以对不同的分量方向的步长做出调整。具体而言，利用 Gk=∑ki=1gi⊙gi 记录分量梯度的累计，并构造如下迭代格式

xk+1=Gk+1=xk−αGk+ϵ1n√⊙gk,Gk+gk+1⊙gk+1.
输入信息：迭代初始值 x0 ，数据集大小 N ，样本梯度计算函数 pgfun ，目标函数值与梯度计算函数 fun 以及提供算法参数的结构体 opts 。

输出信息：迭代得到的解 x 和包含迭代信息的结构体 out 。

out.fvec ：迭代过程中的目标函数值信息
out.nrmG ：迭代过程中的梯度范数信息
out.epoch ：迭代过程中的时期 (epoch)信息
function [x,out] = Adagrad(x0,N,pgfun,fun,opts)
从输入的结构体 opts 中读取参数或采取默认参数。

opts.maxit ：最大迭代次数
opts.alpha ：初始步长
opts.thres ：增强数值稳定性的小量
outs.batchsize ：随机算法的批量大小
opts.verbose ：不为 0 时输出每步迭代信息，否则不输出
if ~isfield(opts, 'maxit'); opts.maxit = 5000; end
if ~isfield(opts, 'alpha'); opts.alpha = 1e-2; end
if ~isfield(opts, 'thres'); opts.thres = 1e-7; end
if ~isfield(opts, 'batchsize'); opts.batchsize = 10; end
if ~isfield(opts, 'verbose'); opts.verbose = 1; end
以 x0 为迭代初始点。 计算初始点处的目标函数值和梯度，记初始时期 (epoch) 为 0。

x = x0;
out = struct();
[f,g] = fun(x);
out.fvec = f;
out.nrmG = norm(g,2);
out.epoch = 0;
gsum 用来存储梯度分量的累计量。 count 用于计算时期 (epoch)。

gsum = zeros(size(x));
count = 1;
迭代主循环
AdaGrad 的迭代循环，以 opts.maxit 为最大迭代次数。

for k = 1:opts.maxit
等概率地从 {1,2,…,N} 中选取批量 sk 记录在 idx 之中，批量大小为 opts.batchsize 。计算对应的样本的梯度。累计梯度分量 Gk=Gk−1+gk⊙gk。

    idx = randi(N,opts.batchsize,1);
    g = pgfun(x,idx);
    gsum = gsum + g.*g;
迭代格式，用 Gk+1 来确定逐分量步长，在下降更快的方向的步长减小， 而下降更慢的方向以更大的步长进行更新。

xk+1=xk−αGk+ϵ1n−−−−−−−−√⊙gk.
    x = x - opts.alpha./sqrt(gsum + opts.thres).*g;
每当参与迭代的总样本次数超过数据集的总样本时，记为一个时期 (epoch)。每一个时期， 记录当前的目标函数值和梯度范数，并令时期计数加一。

    if k*opts.batchsize/N >= count
        [f,g] = fun(x);
        out.fvec = [out.fvec; f];
        out.nrmG = [out.nrmG; norm(g,2)];
        out.epoch = [out.epoch; k*opts.batchsize/N];
        count = count + 1;
    end
end
end
参考页面
在页面 实例：利用随机算法求解逻辑回归问题 中， 我们展示了该算法的一个应用，并且与其它随机算法进行比较。

# AdaDelta
AdaDelta
考虑优化问题：

minx∈Rnf(x)=1N∑i=1Nfi(x).
AdaDelta 算法基于 RMSProp 算法，对迭代增量 Δx（相邻两步迭代的差值）做累计加权和。迭代格式为

Mk=Δxk=Dk=xk+1=ρMk−1+(1−ρ)gk⊙gk;−Dk−1+ϵ1n−−−−−−−−−√Mk+ϵ1n−−−−−−−−√⊙gk;ρDk−1+(1−ρ)Δxk⊙Δxk;xk+Δxk.
目录
初始化和迭代准备
迭代主循环
参考页面
版权声明
初始化和迭代准备
输入信息：迭代初始值 x0 ，数据集大小 N ，样本梯度计算函数 pgfun ，目标函数值与梯度计算函数 fun 以及提供算法参数的结构体 opts 。

输出信息：迭代得到的解 x 和包含迭代信息的结构体 out 。

out.fvec ：迭代过程中的目标函数值信息
out.nrmG ：迭代过程中的梯度范数信息
out.epoch ：迭代过程中的时期 (epoch)信息
function [x,out] = AdaDelta(x0,N,pgfun,fun,opts)
从输入的结构体 opts 中读取参数或采取默认参数。

opts.maxit ：最大迭代次数
opts.thres ：增强数值稳定性的小量
opts.rho ：分量累积的权重
outs.batchsize ：随机算法的批量大小
opts.verbose ：不为 0 时输出每步迭代信息，否则不输出
if ~isfield(opts, 'maxit'); opts.maxit = 5000; end
if ~isfield(opts, 'thres'); opts.thres = 1e-7; end
if ~isfield(opts, 'rho'); opts.rho = 0.95; end
if ~isfield(opts, 'batchsize'); opts.batchsize = 10; end
if ~isfield(opts, 'verbose'); opts.verbose = 1; end
以 x0 为迭代初始点。计算初始点处的目标函数值和梯度，记初始时刻时期 (epoch) 为 0。

x = x0;
out = struct();
[f,g] = fun(x);
out.fvec = f;
out.nrmG = norm(g,2);
out.epoch = 0;
gsum 用来存储梯度分量的累计量。 xsum 用来存储增量分量的累积量。 count 用于计算时期 (epoch)。

gsum = zeros(size(x));
xsum = gsum;
rho = opts.rho;
count = 1;
迭代主循环
AdaGrad 的迭代循环，以 opts.maxit 为最大迭代次数。

for k = 1:opts.maxit
等概率地从 {1,2,…,N} 中选取批量 sk 记录在 idx 之中，批量大小为 opts.batchsize。计算对应的样本的梯度。

    idx = randi(N,opts.batchsize,1);
    g = pgfun(x,idx);
利用梯度分量累计量和增量分量累计对当前步的步长进行修正。注意到这里由于当前步的 Δxk 仍未算出，使用的是 Δxk−1。

    gsum = rho*gsum + (1-rho)*(g.*g);
    dx = - sqrt(xsum + opts.thres)./sqrt(gsum + opts.thres).*g;
    xsum = rho*xsum + (1-rho)*(dx.*dx);
    x = x + dx;
每当参与迭代的总样本次数超过数据集的总样本时，记为一个时期 (epoch)。每一个时期， 记录当前的目标函数值和梯度范数，并令时期计数加一。

    if k*opts.batchsize/N >= count
        [f,g] = fun(x);
        out.fvec = [out.fvec; f];
        out.nrmG = [out.nrmG; norm(g,2)];
        out.epoch = [out.epoch; k*opts.batchsize/N];
        count = count + 1;
    end
# ADAM
Adam 算法
考虑优化问题：

minx∈Rnf(x)=1N∑i=1Nfi(x).
Adam 算法本质上是在 RMSProp 的基础上增加了动量项，其利用梯度的一阶矩记录动量

Sk=ρ1Sk−1+(1−ρ)gk,
记录梯度的二阶矩（与 RMSProp 相同）

Mk=ρ2Mk−1+(1−ρ)gk⊙gk,
并进行修正： Sk^=Sk1−ρk1, Mk^=Mk1−ρk2。 利用修正的一阶矩作为下降方向，并且利用修正的二阶矩来逐分量调整步长，其迭代格式为

xk+1=xk−αMk^+ϵ1n−−−−−−−−√⊙Sk^.
目录
初始化和迭代准备
迭代主循环
参考页面
版权声明
初始化和迭代准备
输入信息：迭代初始值 x0 ，数据集大小 N ，样本梯度计算函数 pgfun，目标函数值与梯度计算函数 fun 以及提供算法参数的结构体 opts 。

输出信息：迭代得到的解 x 和包含迭代信息的结构体 out 。

out.fvec ：迭代过程中的目标函数值信息
out.nrmG ：迭代过程中的梯度范数信息
out.epoch ：迭代过程中的时期 (epoch)信息
function [x,out] = Adam(x0,N,pgfun,fun,opts)
从输入的结构体 opts 中读取参数或采取默认参数。

opts.maxit ：最大迭代次数
opts.alpha ：步长
outs.thres ：保证梯度分量累计严格为正的小量
opts.rho1 ：一阶矩累计的权重值
opts.rho2 ：二阶矩累计的权重值
opts.batchsize ：随机算法的批量大小
opts.verbose ：不小于 1 时输出每步迭代信息，否则不输出
if ~isfield(opts, 'maxit'); opts.maxit = 5000; end
if ~isfield(opts, 'alpha'); opts.alpha = 1e-3; end
if ~isfield(opts, 'thres'); opts.thres = 1e-7; end
if ~isfield(opts, 'rho1'); opts.rho1 = 0.9; end
if ~isfield(opts, 'rho2'); opts.rho2 = 0.999; end
if ~isfield(opts, 'batchsize'); opts.batchsize = 10; end
if ~isfield(opts, 'verbose'); opts.verbose = 1; end
以 x0 为迭代初始点。 计算初始点处的目标函数值和梯度，记初始时刻时期 (epoch) 为 0。

x = x0;
out = struct();
[f,g] = fun(x);
out.fvec = f;
out.nrmG = norm(g,2);
out.epoch = 0;
gsum 记录一阶矩，|ssum| 记录二阶矩。$\rho_1$, ρ2分别为一阶矩和二阶矩的衰减率。 count 用于计算时期(epoch)。

gsum = zeros(size(x));
ssum = gsum;
rho1 = opts.rho1;
rho2 = opts.rho2;
count = 1;
迭代主循环
Adam 的迭代循环，以 opts.maxit 为最大迭代次数。

for k = 1:opts.maxit
等概率地从 {1,2,…,N} 中选取批量 sk 记录在 idx 之中，批量大小为 opts.batchsize 。计算对应的样本的梯度。

    idx = randi(N,opts.batchsize,1);
    g = pgfun(x,idx);
更新一阶、二阶矩累计，并进行修正。 利用修正的一阶矩和二阶矩对 x 进行更新。

    ssum = rho1*ssum +  (1 - rho1)*g;
    gsum = rho2*gsum + (1 - rho2)*(g.*g);
    ssum_mod = ssum/(1 - rho1^k);
    gsum_mod = gsum/(1 - rho2^k);

    x = x - opts.alpha./sqrt(gsum_mod + opts.thres).*ssum_mod;
每当参与迭代的总样本次数超过数据集的总样本时，记为一个时期 (epoch)。每一个时期， 记录当前的目标函数值和梯度范数，并令时期计数加一。

    if k*opts.batchsize/N >= count
        [f,g] = fun(x);
        out.fvec = [out.fvec; f];
        out.nrmG = [out.nrmG; norm(g,2)];
        out.epoch = [out.epoch; k*opts.batchsize/N];
        count = count + 1;
    end
end
end
