陈静茹-20220513-作业12

# 分布式聚类

## one-hot representation不足

- 向量的维度会随着句子的词的数量类型增大而增大；

- “词汇鸿沟”现象：任意两个词之间都是孤立的，根本无法表示出在语义层面上词语词之间的相关信息

## 词的分布式表示distributed representation

原文链接：https://blog.csdn.net/qq_40136685/article/details/90581435

简介
Harris 在 1954 年提出的分布假说（ distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。Firth 在 1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（ a word is characterized by the company it keeps）。

分布式表示是一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177, −0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的。

分布式表示最大的贡献就是让相关或者相似的词，在距离上更接近了，在一定程度上解决了独热表示的维度和词汇鸿沟问题。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。
## 建模类型

### 基于矩阵的分布表示

基于矩阵的分布表示通常又称为分布语义模型，这类方法需要构建一个“词-上下文”矩阵，从矩阵中获取词的表示。在“词-上下文”矩阵中，每行对应一个词，每列表示一种不同的上下文，矩阵中的每个元素对应相关词和上下文的共现次数。在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。

由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。

### 基于聚类的分布表示

该方法以根据两个词的公共类别判断这两个词的语义相似度。最经典的方法是布朗聚类（Brown clustering）。

### 基于神经网络的分布表示

词嵌入（word embedding）基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）。

# 布朗聚类

布朗聚类是一种自底层向上的层次聚类算法，基于n-gram模型和马尔科夫链模型。布朗聚类是一种硬聚类，每一个词都在切只在唯一的一个类中。

![image-20220517170616501](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517170616501.png)

布朗聚类的输入是一个语料库，这个语料库是一个词序列，输出是一个二叉树，树的叶子节点是一个个词，树的中间节点是类别(中间节点作为根节点的子树上的所有叶子为类中的词)。

![image-20220517170655779](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517170655779.png)

初始的时候，将每一个词独立分成一类，然后，将两个类合并，使得合并之后评价函数最大，然后不断重复上述过程，达到想要的类别数量为止。

评价函数：对于n个连续的词(W)序列能否组成一句话的概率的对数的归一化结果。于是，得到评价函数：

![image-20220517170717794](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517170717794.png)

其中：n是文本长度，w是词

# Word2Vec

Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型。Word2Vec其实就是通过学习文本来用词向量的方式表征词的语义信息，即通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。

Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。

![preview](https://pic1.zhimg.com/v2-1f525e2259edd9210fe470ec7fc218d4_r.jpg)



## CBOW（以下内容纯属抄袭，没懂）

1. 单个上下文情境

我们先来介绍CBOW模型中最简单的一个版本。假定每一上下文只有一个单词，也就是模型通过给定一个上下文单词来预测一个目标单词，就好比一个二元模型。

图 1 展示了该简化版模型的神经网络结构。根据我们的设定，模型词汇表的大小是V，每个隐藏层的维度是N。相邻的神经元之间的连接方式是全连接。

![img](https://pic3.zhimg.com/80/v2-52be7b29eb86ac796ef078d42e220f1e_720w.jpg)



模型的输入是一个独热（one-hot）向量，这意味着对于给定的一个单词，![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+x1%EF%BC%8C%E2%80%A6%EF%BC%8CxV%5Cright%5C%7D) 这个序列中只有一个值为1，其他值都为0。输入层和隐藏层之间的权重矩阵可以用一个 ![[公式]](https://www.zhihu.com/equation?tex=V%5Ctimes+N) 的矩阵W来表示。W的每一行是一个与模型输入相对应的N维向量 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D) ，经过转置，矩阵的第i行就是 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7BT%7D) 。给定一上下文（也就是一个单词），假定输入序列中![[公式]](https://www.zhihu.com/equation?tex=x_%7Bk%7D%5E%7B%7D) ![[公式]](https://www.zhihu.com/equation?tex=%3D) 1和 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bk%7D%5E%7B%27%7D%3D0) （对所有的 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bk%7D%5E%7B%27%7D%5Cne+x_%7Bk%7D) ），可以得到：

![img](https://pic3.zhimg.com/80/v2-9dd65cd28858b00b99284afa0125c7f2_720w.png)

也就是直接复制W矩阵第k行赋给h， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D) 是输入单词wi的向量表示。这表明隐藏层单元的激活函数是简单的线性关系（也就是传递输入向量的加权和给下一层）。

隐藏层和输出层之间，存在一个不同的矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%E2%80%99%7D) ，该矩阵的维度是 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+V) 。有了这些权重，我们可以计算出词汇表中每个单词 ![[公式]](https://www.zhihu.com/equation?tex=u_%7Bj%7D) 的得分：

![img](https://pic4.zhimg.com/80/v2-46276488d7890eb133f801e5ae80b633_720w.png)

![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwj%7D%5E%7B%27%7D) 是矩阵 ![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%27%7D) 的第j列。现在可以用softmax，一种对数线性分类模型，去得到单词的后验分布，它是一种多项式分布。

![img](https://pic2.zhimg.com/80/v2-c5ecb93b3466b8215833f818658ed071_720w.png)

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D) 是输出向量中第j个单元。把式（1）和式（2）带入式（3），可得：

![img](https://pic3.zhimg.com/80/v2-0e6b0ac9f7d58afd44603d21817aca5a_720w.jpg)

注意一点， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D) 是对单词w的两种不同的向量表示。 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D) 来自于矩阵W的行向量，也就是输入层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow)隐藏层的权重矩阵， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%E2%80%99%7D) 来自于矩阵 的列向量，也就是隐藏层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 输出层的权重矩阵。以下，我们将 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D) 称为单词w的输入向量， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D) 称为单词w的输出向量。（补充一下，这两类向量，有的文章中也叫做中心词向量和上下文向量）



**隐藏层** ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) **输出层权重的更新公式**

现在我们来推导一下该模型的权重更新公式。尽管该公式实际计算中并不可行（下面会具体解释原因），我们将直接使用这个原始模型（不加任何优化技巧）来演绎一下推导过程以增加对该模型的理解。对于反向传播的一些基础概念，可以参考附录A。

该模型的训练目标是最大化公式（4），给定输入wi下真实输出值 ![[公式]](https://www.zhihu.com/equation?tex=w_%7BO%7D) 的条件概率（正确输出单词的索引是 ![[公式]](https://www.zhihu.com/equation?tex=j%5E%7B%2A%7D) ）。



![img](https://pic3.zhimg.com/80/v2-1181ff28032ef62b3cded13d58947d06_720w.jpg)



损失函数 ![[公式]](https://www.zhihu.com/equation?tex=E%3D-log+P%28w_%7BO%7D%7C+w_%7BI%7D%29) （我们的目标是最小化 E）， ![[公式]](https://www.zhihu.com/equation?tex=j%5E%7B%2A%7D) 是真实输出值的索引。该损失函数可以理解为一种特殊的衡量两个概率分布的交叉熵形式。

现在我们来求解隐藏层和输出层权重更新方程，求E对求 ![[公式]](https://www.zhihu.com/equation?tex=u_%7Bj%7D) 的偏导，可得：



![img](https://pic1.zhimg.com/80/v2-b6b651675575a25419a511a84e9e4ea0_720w.jpg)



当 ![[公式]](https://www.zhihu.com/equation?tex=j%3Dj%5E%7B%2A%7D) 时， ![[公式]](https://www.zhihu.com/equation?tex=t_%7Bj%7D) =1，否则为0。可以看出，这个结果恰好等于**预测误差** ![[公式]](https://www.zhihu.com/equation?tex=e_%7Bj%7D) 。

接下来我们对 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bij%7D%5E%7B%27%7D) 求偏导，得到隐藏层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 输出层权重的梯度：



![img](https://pic4.zhimg.com/80/v2-605fc7634cfc31a619c9de0d44635abf_720w.jpg)





因此，使用随机梯度下降的方法，我们可以得到隐藏层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 输出层的权重更新公式：



![img](https://pic2.zhimg.com/80/v2-7d5edeb4e2e2a44fce8dec7d402806f5_720w.jpg)



学习率 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) >0 , ![[公式]](https://www.zhihu.com/equation?tex=e_%7Bj%7D%3Dy_%7Bj%7D-t_%7Bj%7D) , ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bj%7D) 是隐藏层的第j个单元； ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwj%7D%5E%7B%27%7D) 是单词 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D) 的输出向量。注意到该更新公式意味着我们必须遍历词汇表中的所有单词来才能得到输出值 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D) ，再将 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D) 和期望输出 ![[公式]](https://www.zhihu.com/equation?tex=t_%7Bj%7D) （0或1）进行比对。如果 ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D%3Et_%7Bj%7D) （结果估计过高），那么我们让 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwj%7D%5E%7B%27%7D) 减去一定比例的隐向量**h**（也就是 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D) ）,这样使得向量![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwj%7D%5E%7B%27%7D) 更加远离向量 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D)；如果![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D%3Ct_%7Bj%7D)(结果被低估，仅当 ![[公式]](https://www.zhihu.com/equation?tex=t_%7Bj%7D) =1时发生的情况，此时 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D%3Dw_%7BO%7D) )，我们加上一定比例的h给 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwo%7D%5E%7B%27%7D) ，使得![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwo%7D%5E%7B%27%7D)更接近于![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D%5E%7B%27%7D)。如果![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D%E5%92%8Ct_%7Bj%7D)非常接近，根据公式，权重的更新值也会很小。**再次提醒，输入向量** ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D) **和输出向量** ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D) **是对同一个单词w的两种不同的表达。**



**输入层** ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) **隐藏层权重的更新公式**

有了 ![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%27%7D) 的更新公式，我们继续求解W的更新公式。我们求E对 ![[公式]](https://www.zhihu.com/equation?tex=h_%7Bi%7D) 的偏导，得到：



![img](https://pic2.zhimg.com/80/v2-96589f07873fdf41eef766c0bece2c79_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=h_%7Bi%7D)是隐藏层第i个单元的输出；根据公式（2）的定义， ![[公式]](https://www.zhihu.com/equation?tex=u_%7Bj%7D)是输出层的第j个输入；![[公式]](https://www.zhihu.com/equation?tex=e_%7Bj%7D%3Dy_%7Bj%7D-t_%7Bj%7D)是输出层第j个单词的预测误差。EH是一个N维向量，是词汇表所有单词的输出向量根据预测误差的加权求和。

接下来，我们来求E对输入权重矩阵W的偏导。首先，让我们回忆一下，隐藏层是对输入层的值的线性计算过程。把公式（1）展开得到：



![img](https://pic1.zhimg.com/80/v2-43b035a244f5250dd9d194b08f920bd4_720w.jpg)



现在我们可以对W的各元素进行求导，得到：



![img](https://pic1.zhimg.com/80/v2-64adb8c59541430bcd334b0d9d359fb8_720w.jpg)



这和x和EH的张量计算输出等价，



![img](https://pic4.zhimg.com/80/v2-377fb93cd9f96769946952554eb40e63_720w.jpg)



我们最终得到了一个V*N的矩阵。因为x向量中只有一个元素是非0的，所有 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpartial+E%2F%5Cpartial+W)

矩阵中只有一行是非0，该行的值为EH(x的值为1)，是一个N维向量。所以最终W的更新公式为：



![img](https://pic2.zhimg.com/80/v2-58eb2f14ebbbef2daff177932441ddfd_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D) 是唯一上下文单词的输入向量，也是W矩阵的导数中唯一为非0的一行。W矩阵的其他行在迭代过程中均保持不变，因为他们的导数都为0.

直观的来理解一下这个过程，由于EH向量是词汇表中所有单词的输出向量根据预测误差![[公式]](https://www.zhihu.com/equation?tex=e_%7Bj%7D%3Dy_%7Bj%7D-t_%7Bj%7D)的加权求和。我们可以这样理解公式（16），将输出向量的一部分加到上下文单词的输入向量中去。如果在输出层，单词 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D) 作为预测单词的概率被高估（![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D%3Et_%7Bj%7D)），那么上下文单词wi的输入向量趋向于远离![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)的输出向量；相反的，如果单词![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)作为预测单词的概率被低估（![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D%3Ct_%7Bj%7D)）,那么上下文单词wi的输入向量将趋向于靠近![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)的输出向量；如果单词

![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D)作为预测单词恰好等于真实值，那么上下文单词wi的输入向量将几乎没有什么改变。输入向量wi的变化情况取决于词汇表中所有单词向量的预测误差；误差越大，输入向量的变动越大。

由于在训练过程中，我们是通过训练语料中的上下文—目标单词对来迭代地更新模型参数，每次迭代更新对向量的影响也是累积的。我们可以想象成单词w的输出向量被w所有周围词的输入向量的来回往复的拖拽。就好比有真实的琴弦在单词w和其他词之间。同样的，输入向量也可以被想象成被很多输出向量拖拽。这种解释可以提醒我们想象成一个重力，或者其他力量所指引的输出图。每根弦的均衡长度跟共同出现的关联单词对之间的力量权衡有关，也跟学习率有关。经过多次迭代，输入和输出向量的相对位置将趋于平稳。



1. **多个单词上下文情境**

图2展示了多个上下文单词的CBOW模型。当计算隐藏层的输出时，CBOW并不是直接拷贝上下文的输入向量，而是对上下文单词的输入向量求均值后再作为输出。



![img](https://pic4.zhimg.com/80/v2-12b76690d3a57fd9301a7f2eb671643b_720w.jpg)





C是上下文的单词个数， ![[公式]](https://www.zhihu.com/equation?tex=w_%7B1%7D%2C+%5Ccdot%5Ccdot%5Ccdot%2Cw_%7BC%7D) 是上下文中的单词， ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D) 是单词w的输入向量。损失函数定义为：



![img](https://pic2.zhimg.com/80/v2-fa4e707ac56bcf5963db065ac193a181_720w.jpg)



跟公式（7）一样，目标和one-word-context模型一致，除了h的定义不同，也就是将公式（18）替换公式（1）。



![img](https://pic4.zhimg.com/80/v2-73885c25dd5d7cbbba9e7834b18c0aa3_720w.jpg)



对于隐藏层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 输出层，梯度更新公式跟one-word-context模型一致。直接拷贝公式如下：



![img](https://pic3.zhimg.com/80/v2-4c3955565ca5baf71795631c473eae26_720w.jpg)



注意一点在训练过程中我们需要把该公式迭代作用于输出矩阵的每个元素。对于输入层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 隐藏层，梯度更新公式和式（16）类似，除了现在需要将如下公式作用于上下文词的每个单词 ![[公式]](https://www.zhihu.com/equation?tex=w_%7BI%2Cc%7D) ：



![img](https://pic4.zhimg.com/80/v2-3903ac953b04b387d1f62e0ea2add353_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=w_%7BI%2Cc%7D)是上下文中第c个单词的输入向量； ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta) 是学习率；可EH由公式（12）得到。对更新公式的直观理解可以参考（16）。



## Skip-Gram Model

Skip-gram模型是由Mikolov等人提出的。图3展示了skip-gram模型的过程。该模型可以看做是CBOW模型的逆过程。CBOW模型的目标单词在该模型中作为输入，上下文则作为输出。

我们仍然使用 ![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwi%7D) 来表示输入层中唯一单词（也叫中心词）的输入向量，所以这样的话，对隐藏层h的定义跟公式（1）一致，意味着h仅仅只是简单拷贝了输入层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 隐藏层的权重矩阵W

中跟输入单词wi相关的那一行。拷贝公式得到：



![img](https://pic1.zhimg.com/80/v2-d44fb4fcafadac7d3d2874390ac45c34_720w.jpg)



在输出层，我们输出C个多项式分布来替代仅输出一个多项式分布。每个输出是由同一个隐藏层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 输出层矩阵计算得出的：



![img](https://pic1.zhimg.com/80/v2-e67131a4c545ab7064e08d258bba569c_720w.jpg)



这里 ![[公式]](https://www.zhihu.com/equation?tex=w_%7Bc%2Cj%7D) 是第c个输出面上第j个单词；![[公式]](https://www.zhihu.com/equation?tex=w_%7BO%2Cc%7D)是中心词对应的目标单词中的第c个单词；wi是中心词（唯一输入单词）；![[公式]](https://www.zhihu.com/equation?tex=y_%7Bc%2Cj%7D)是第c个输出面上第j个单元的输出值；![[公式]](https://www.zhihu.com/equation?tex=u_%7Bc%2Cj%7D)是第c个输出面上的第j个单元的输入。因为输出面共享同一权重矩阵，所以有：



![img](https://pic2.zhimg.com/80/v2-180be00c6ad4e502c221c567e9a283c9_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=v_%7Bwj%7D%5E%7B%27%7D) 是词汇表第j个单词的输出向量，可由 ![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%27%7D) zsssss1s1阵中的所对应的一列拷贝得到。



![img](https://pic3.zhimg.com/80/v2-0ed8b9efc8aa7867fbf001b921cec4ee_720w.jpg)



模型参数的更新过程和one-word-context模型基本一致。损失函数更改为：



![img](https://pic3.zhimg.com/80/v2-d7b6b1b841299b6f7e037d0b05e1731a_720w.jpg)





![[公式]](https://www.zhihu.com/equation?tex=j_%7Bc%7D%5E%7B%2A%7D)是词汇表中第c个真实输出单词的索引。我们求E对![[公式]](https://www.zhihu.com/equation?tex=u_%7Bcj%7D%5E%7B%7D)的偏导，得到：



![img](https://pic1.zhimg.com/80/v2-87f14a487cf6db59c65d23ddf17f6b64_720w.jpg)



跟公式（8）一致，这个就是该单元的预测误差。为了方便表示，我们定义一个V维向量 ![[公式]](https://www.zhihu.com/equation?tex=EI%3D%5Cleft%5C%7B+EI_%7B1%7D+%2C%5Ccdot%5Ccdot%5Ccdot%2C+EI_%7BV%7D%5Cright%5C%7D) ，该向量是C个预测单词的误差总和：



![img](https://pic3.zhimg.com/80/v2-4eab584e418132b7f4282584adec8cf2_720w.jpg)



接下来，我们作E对 ![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%27%7D) （隐藏层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 输出层的权重矩阵）的偏导，可得：



![img](https://pic4.zhimg.com/80/v2-7fcd21a077d87bda49599e1e9b4ed4d3_720w.jpg)





这样我们得到了隐藏层到输出层矩阵![[公式]](https://www.zhihu.com/equation?tex=W%5E%7B%27%7D)的梯度更新公式，



![img](https://pic4.zhimg.com/80/v2-c63d9610b3e7068c72bb91813a017447_720w.jpg)



直观上对该公式的理解跟（11）一样，除了预测误差是对输出层所有上下文词的误差总和。

对输入层 ![[公式]](https://www.zhihu.com/equation?tex=%5Crightarrow) 隐藏层的权重更新跟（12）和（16）一致，除了需要将预测误差![[公式]](https://www.zhihu.com/equation?tex=e_%7Bj%7D%5E%7B%7D)替换为![[公式]](https://www.zhihu.com/equation?tex=EI_%7Bj%7D%5E%7B%7D)，直接给出更新公式：



![img](https://pic2.zhimg.com/80/v2-c1ac11335701c64c8157b1d4ca41cd79_720w.jpg)





EH是一个N维向量，向量每个单元被定义为：



![img](https://pic2.zhimg.com/80/v2-12dbb71018d739dc0e13cc48cb3fb611_720w.jpg)



直觉上的理解和（16）一致。



1. **模型的优化方法**

以上我们讨论的模型（二元模型，CBOW和skip-gram）都是他们的原始形式，没有加入任何优化技巧。

对于这些模型，每个单词存在两类向量表达：输入向量![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%7D)，输出向量![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D)（这也是为什么word2vec的名称由来：1个单词对应2个向量表示）。学习得到输入向量比较简单；但要学习输出向量是很困难的。从公式（22）和（33）得知，为了更新![[公式]](https://www.zhihu.com/equation?tex=v_%7Bw%7D%5E%7B%27%7D)，在每次训练中，我们必须遍历词汇表中的每个单词![[公式]](https://www.zhihu.com/equation?tex=w_%7Bj%7D%5E%7B%7D)，从而计算得到 ![[公式]](https://www.zhihu.com/equation?tex=u_%7Bj%7D%5E%7B%7D)，预测概率![[公式]](https://www.zhihu.com/equation?tex=y_%7Bj%7D%5E%7B%7D)（skip-gram为![[公式]](https://www.zhihu.com/equation?tex=y_%7Bc%2Cj%7D%5E%7B%7D)），它们的预测误差![[公式]](https://www.zhihu.com/equation?tex=e_%7Bj%7D%5E%7B%7D)，（skip-gram为![[公式]](https://www.zhihu.com/equation?tex=EI_%7Bj%7D%5E%7B%7D)），然后再用误测误差来更新输出向量![[公式]](https://www.zhihu.com/equation?tex=v_%7Bj%7D%5E%7B%27%7D)。

对每个训练过程做如此庞大的计算是非常昂贵的，使得它难以扩展到词汇表或者训练样本很大的任务中去。为了解决这个问题，我们直观的想法就是限制每次必须更新的输出向量的数量。一种有效的手段就是采用**分层softmax**；另一种可行的方法是通过**负采样**。

这两种技巧都只针对**输出向量**更新的优化。在我们的推导过程中，我们关心三种值：（1）

E，新的目标函数；（2） ![[公式]](https://www.zhihu.com/equation?tex=%5Cpartial+E%2F%5Cpartial+v_%7Bw%7D%5E%7B%27%7D) ，新的输出向量的更新公式；（3）![[公式]](https://www.zhihu.com/equation?tex=%5Cpartial+E%2F%5Cpartial+h_%7B%7D%5E%7B%7D)，反向传播中用于更新输入向量的预测误差的加权和。

# 矩阵分解

## 奇异值分解(Singular Value Decomposition，以下简称SVD）

链接：https://zhuanlan.zhihu.com/p/29846048

特征分解：

首先回顾下特征值和特征向量的定义如下：

![[公式]](https://www.zhihu.com/equation?tex=Ax%3D%5Clambda+x) 

其中 ![[公式]](https://www.zhihu.com/equation?tex=A) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=n%5Ctimes+n) 矩阵， ![[公式]](https://www.zhihu.com/equation?tex=x) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=n) 维向量，则 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 是矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A) 的一个特征值，而 ![[公式]](https://www.zhihu.com/equation?tex=x) 是矩阵 ![[公式]](https://www.zhihu.com/equation?tex=A) 的特征值 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 所对应的特征向量。

求出特征值和特征向量有什么好处呢？ 就是我们可以将矩阵A特征分解。如果我们求出了矩阵A的n个特征值 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_%7B1%7D%5Cleq+%5Clambda_%7B2%7D%5Cleq...+%5Cleq+%5Clambda_%7Bn%7D) ，以及这 ![[公式]](https://www.zhihu.com/equation?tex=n) 个特征值所对应的特征向量 ![[公式]](https://www.zhihu.com/equation?tex=%7B%7B+w_%7B1%7D%2Cw_%7B2%7D%7D%2C...%2Cw_%7Bn%7D%7D) ，

那么矩阵A就可以用下式的特征分解表示：

![image-20220517174101172](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517174101172.png)

注意到要进行特征分解，矩阵A必须为方阵。

那么如果A不是方阵，即行和列不相同时，我们还可以对矩阵进行分解吗？答案是可以，此时我们的SVD登场了。

SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设我们的矩阵A是一个m×n的矩阵，那么我们定义矩阵A的SVD为：

![image-20220517174229199](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517174229199.png)

其中 ![[公式]](https://www.zhihu.com/equation?tex=U) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=m%5Ctimes+m) 的矩阵， ![[公式]](https://www.zhihu.com/equation?tex=%5CSigma) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=m%5Ctimes+n) 的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值， ![[公式]](https://www.zhihu.com/equation?tex=V) 是一个 ![[公式]](https://www.zhihu.com/equation?tex=n%5Ctimes+n) 的矩阵。 ![[公式]](https://www.zhihu.com/equation?tex=U) 和 ![[公式]](https://www.zhihu.com/equation?tex=V) 都是酉矩阵，即满足

![image-20220517174308426](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517174308426.png)



U，![[公式]](https://www.zhihu.com/equation?tex=%5CSigma)，V的求解涉及大量手推公式，感兴趣的可以戳链接了解详情。

## CCA

链接：https://www.zhihu.com/question/52381656/answer/969115054

在数理统计里面，我们都知道相关系数这个概念。假设有两组一维的数据集X和Y，则相关系数ρ的定义为:

![image-20220517174725769](C:\Users\pepper\AppData\Roaming\Typora\typora-user-images\image-20220517174725769.png)

其中cov(X,Y)是X和Y的协方差，而D(X),D(Y)分别是X和Y的方差。相关系数ρ的取值为[-1,1],ρ的绝对值越接近于1，则X和Y的线性相关性越高。越接近于0，则X和Y的线性相关性越低。

虽然相关系数可以很好的帮我们分析一维数据的相关性，但是对于高维数据就不能直接使用了。

CCA使用的方法是将多维的X和Y都用线性变换为1维的X'和Y'，然后再使用相关系数来看X'和Y'的相关性。将数据从多维变到1位，也可以理解为CCA是在进行降维，将高维数据降到1维，然后再用相关系数进行相关性的分析。

CCA算法同样涉及大量手推公式，数学推导，请自行戳链接学习。

## PCA

主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。

首先考虑一个问题：对于正交属性空间中的样本点，如何用一个[超平面](https://www.zhihu.com/search?q=超平面&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"32412043"})（直线的高维推广）对所有样本进行恰当的表达？

可以想到，若存在这样的超平面，那么它大概具有这样的性质：

- 最近重构性：样本点到这个超平面的距离足够近
- 最大可分性：样本点在这个超平面上的投影能尽可能的分开

基于最近重构性和最大可分性能分别得到主成分分析的两种等价推导。


链接：https://zhuanlan.zhihu.com/p/32412043


讲的很通俗易懂的：
链接：https://www.zhihu.com/question/41120789/answer/481966094