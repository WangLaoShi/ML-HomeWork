#  自适应学习率算法
## 自适应学习率算法 —— AdaGrad
AdaGrad是Adaptive Gradient的缩写，适应方法是每个参数都有自己的学习率，这里的学习率是和每个参数的梯度相关的，而且是累积的；算出一个参数的梯度之后，会去计算累积的平方梯度，如果这个参数已经被更新了多次，二阶动量大，那么学习率就小；反之更新少的参数就会有一个比较大一些的学习率去更新
![](https://s2.loli.net/2022/05/21/Vh3FEzyS6MTnrwj.png)
问题：第一个是一直累积动量累积太大后面就更新不动了，另一个是还是要依靠epsilon的设置，设置比较大的话还是会有影响。
##  自适应学习率算法 —— RMSprop
RMSprop是AdaGrad的一种变体，就是将动量累积和当前时刻的梯度做了一个加权求和（滑动平均），这么做的目的是为了让之前的梯度对当前影响变小
![](https://s2.loli.net/2022/05/21/jD2O1vEXxMFZlmW.png)
## 自适应学习率算法 —— Adadelta
Adadelta和RMSprop一样就是做了一个加权求和（滑动平均），但是不一样的是用近段的一个时间窗口内的动量累积，而不是历史动量累积，这就更能避免AdaGrad的第一个问题。
## 自适应学习率算法 —— Adam(Adaptive Moment Estimation)
Adam其实就是Momentum + RMSprop；算完梯度后，更新一阶矩估计s和二阶矩估计r，s作为梯度动量累积（Momentum），r作为学习率的分母（RMSprop）；最后的公式如果没有r那一坨就是Momentum了，如果把s换成g就是RMSprop。
![](https://s2.loli.net/2022/05/21/mnTzvQFfU98riR3.png)
### Momentum的特点
特点：相比于SGD，考虑了历史累计梯度。
优点：旨在加速学习，特别是高曲率、小但一致的梯度，或是带噪声的梯度。动量算法累积了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。

目前理解只是理解了他的意思，具体内在工作逻辑有一点迷
