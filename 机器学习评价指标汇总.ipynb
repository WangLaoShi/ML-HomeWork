{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习评价指标汇总\n",
    "### 分类\n",
    "#### 精确率与召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC(Receiver operating characteristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.metrics import auc  ###计算roc和auc\n",
    "import pandas as pd\n",
    " \n",
    "base = \"D:\\\\WFLW\\\\wflw_blur_128\\\\ROC\\\\\"\n",
    " \n",
    "df =pd.read_csv( base + \"sobel_roc.txt\", encoding='utf8', delimiter=' ', header=0)\n",
    "df_lap =pd.read_csv( base + \"lap_roc.txt\", encoding='utf8', delimiter=' ', header=0)\n",
    "df_dft =pd.read_csv( base + \"dft_roc.txt\", encoding='utf8', delimiter=' ', header=0)\n",
    "df_cnn =pd.read_csv( base + \"cnn_roc.txt\", encoding='utf8', delimiter=' ', header=0)\n",
    " \n",
    "tpr = df['tpr']\n",
    "fpr = df['fpr']\n",
    " \n",
    "tpr_lap = df_lap['tpr']\n",
    "fpr_lap = df_lap['fpr']\n",
    " \n",
    "tpr_dft = df_dft['tpr']\n",
    "fpr_dft = df_dft['fpr']\n",
    " \n",
    "tpr_cnn = df_cnn['tpr']\n",
    "fpr_cnn = df_cnn['fpr']\n",
    " \n",
    "roc_auc = auc(fpr, tpr) ###计算auc的值\n",
    " \n",
    "roc_auc_lap = auc(fpr_lap, tpr_lap) ###计算auc的值\n",
    " \n",
    "roc_auc_dft = auc(fpr_dft, tpr_dft) ###计算auc的值\n",
    " \n",
    "roc_auc_cnn = auc(fpr_cnn, tpr_cnn)\n",
    " \n",
    "print(roc_auc)\n",
    " \n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='Sobel ROC curve (area = %0.2f)' % roc_auc) ###假正率为横坐标，真正率为纵坐标做曲线\n",
    " \n",
    "plt.plot(fpr_lap, tpr_lap, color='navy', lw=lw, label='Lap ROC curve (area = %0.2f)' % roc_auc_lap)\n",
    " \n",
    "plt.plot(fpr_dft, tpr_dft, color='red', lw=lw, label='Dft ROC curve (area = %0.2f)' % roc_auc_dft)\n",
    " \n",
    "plt.plot(fpr_cnn, tpr_cnn, color='green', lw=lw, label='CNN ROC curve (area = %0.2f)' % roc_auc_cnn)\n",
    "#plt.plot(tpr, fpr, 'k--', label='Mean ROC (area = %0.2f)' % 1, lw=2)\n",
    " \n",
    "#plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    " \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(base + \"roc_img\\\\sobel_roc.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "y = np.array([1, 1, 2, 2])\n",
    "scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对数损失 Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "value=log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],[[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
    "print(\"value:\",value)\n",
    "\n",
    "\n",
    "def logloss(true_label, predicted_prob):\n",
    "  if true_label == 1:\n",
    "    return -np.log(predicted_prob)\n",
    "  else:\n",
    "    return -np.log(1 - predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 铰链损失 Hinge loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import hinge_loss\n",
    "X = [[0], [1]]\n",
    "y = [-1, 1]\n",
    "est = svm.LinearSVC(random_state=0)\n",
    "est.fit(X, y)\n",
    "\n",
    "pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
    "pred_decision\n",
    "\n",
    "hinge_loss([-1, 1, 1], pred_decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 混淆矩阵 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kappa 系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Calculates the kappa\n",
    "    kappa calculates the kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            if i == j:\n",
    "                d = 0.0\n",
    "            else:\n",
    "                d = 1.0\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return 1.0 - numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准确率 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_accuracy(feature_matrix, sentiment, coefficients):\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    apply_threshold = np.vectorize(lambda x: 1. if x > 0  else -1.)\n",
    "    predictions = apply_threshold(scores)\n",
    "    \n",
    "    num_correct = (predictions == sentiment).sum()\n",
    "    accuracy = num_correct / len(feature_matrix)    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'prec': 'precision'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 海明距离 Hamming Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def hamming_distance(v1,v2)\n",
    "smstr=np.nonzero(v1-v2)\n",
    "sm= np.shape(smstr[0])[0]\n",
    "return sm,smstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "y_pred =np.array([[0,1,1,1,0],\n",
    "          [1,0,0,1,1],\n",
    "          [1,1,0,0,0],\n",
    "          [1,0,1,0,1]])\n",
    "\n",
    "y_true =np.array([[1,1,1,0,0],\n",
    "          [1,0,0,1,1],\n",
    "          [1,0,1,0,1],\n",
    "          [1,1,0,1,1]])\n",
    "\n",
    "h=hamming_loss(y_true, y_pred)  \n",
    "print(\"汉明损失：\",h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 杰卡德相似系数 Jaccard similarity coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    " y_true = np.array([[0, 1, 1],\n",
    "...                    [1, 1, 0]])\n",
    "y_pred = np.array([[1, 1, 1],\n",
    "...                    [1, 0, 0]])\n",
    "jaccard_score(y_true[0], y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多标签排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 涵盖误差 Coverage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import coverage_error\n",
    "c=coverage_error(y_true, y_pred)-1\n",
    "print(\"覆盖误差：\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 标签排序平均精度 Label ranking average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "a=average_precision_score(y_true, y_pred) \n",
    "print(\"平均精度损失：\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归\n",
    "#### 平均绝对误差 MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 平均绝对误差\n",
    "def MAELoss(x:list,y:list):\n",
    "    \"\"\"\n",
    "    x:list，代表模型预测的一组数据\n",
    "    y:list，代表真实样本对应的一组数据\n",
    "    \"\"\"\n",
    "    assert len(x)==len(y)\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    loss=np.sqrt(np.sum(np.square(x - y)) / len(x))/n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 平均平方误差 MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def MSELoss(x:list,y:list): #MSE\n",
    "    \"\"\"\n",
    "    x:list，代表模型预测的一组数据\n",
    "    y:list，代表真实样本对应的一组数据\n",
    "    \"\"\"\n",
    "    assert len(x)==len(y)\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    loss=np.sum(np.square(x - y)) / len(x)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)\n",
    ">>> 0.375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解释变异系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "explained_variance_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决定系数 $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R2Loss(x:list,y:list):\n",
    "    \"\"\"\n",
    "    x:list，代表模型预测的一组数据\n",
    "    y:list，代表真实样本对应的一组数据\n",
    "    \"\"\"\n",
    "    assert len(x)==len(y)\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    t=np.sum(y)/len(y)\n",
    "    loss=1-((np.sum(np.square(x - y)) )/(np.sum(np.square(y-t))))\n",
    "    loss=np.sqrt(np.sum(np.square(x - y)) / len(x))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚类\n",
    "#### 兰德指数 Rand index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randIndex(clusters):\n",
    "\n",
    "    answers = np.load('answersMatrixNOVO.npy')\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    d = 0\n",
    "    \n",
    "    N = len(clusters)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(0,N):\n",
    "        for j in range(0,N):\n",
    "            \n",
    "            if answers[i,j] != 0.0:\n",
    "                \n",
    "                count = count + 1\n",
    "            \n",
    "                if clusters[i] == clusters[j]:\n",
    "                    \n",
    "                    if answers[i,j] == 2:\n",
    "                        a = a + 1\n",
    "                    else:\n",
    "                        c = c + 1\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    if answers[i,j] == 1:\n",
    "                        b = b + 1\n",
    "                    else:\n",
    "                        d = d + 1\n",
    "                       \n",
    "    rand_index = float(a + b)/(a + b + c + d)\n",
    "    n = a+b+c+d\n",
    "    aux = float((a+c)*(a+d) + (c+b)*(d+b))\n",
    "    \n",
    "    adjusted_rand_index = float(n*(a+b) - aux)/(pow(n,2) - aux)\n",
    "    \n",
    "    print 'a =' + str(a)\n",
    "    print 'b =' + str(b)\n",
    "    print 'c =' + str(c)\n",
    "    print 'd =' + str(d)\n",
    "    print 'Rand Index = ' + str(rand_index)\n",
    "    print 'Adjusted Rand Index = ' + str(adjusted_rand_index)\n",
    "    print 'Positive-negative coeficient index =' + str((float(a)/4635 + float(b)/15263)/2)\n",
    "    \n",
    "    return adjusted_rand_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "metrics.rand_score(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 互信息 Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels_true = [0, 0, 0, 1, 1, 1]\n",
    "labels_pred = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "metrics.adjusted_mutual_info_score(labels_true, labels_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 轮廓系数 Silhouette coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\n",
    "labels = kmeans_model.labels_\n",
    "metrics.silhouette_score(X, labels, metric='euclidean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
