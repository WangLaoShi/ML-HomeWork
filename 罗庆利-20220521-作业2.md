# 回溯线搜索

无约束最优化中，基于Armijo-Goldstrain条件的线搜索方法，给定搜索方向，逐步缩小步长，直到目标函数与最优值差值在可容纳误差之内。

给定初始位置**x0**和搜索方向**p**，任意α>0，确定步长，使得f(x+αp)<=f(x)+acm

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/9b81efac27c09e1a2fc62eb6e72fc35c58710796)，c为控制参数∈(0,1)

终止条件：

## 算法步骤
从最大候选步长值开始α0 > 0,搜索控制参数 τ∈（0,1）和c ∈（0,1）

step0： t = -cm,迭代计数器 j = 0  
stpe1:满足条件![](https://wikimedia.org/api/rest_v1/media/math/render/svg/591d76a31438b1bcb2fd81760a21a929c7bd1b23),
反复递增j并设置![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2e4774852d79ca2c2a12a2d3a769dc7d3e3d0b9d)  
step2:返回αj作为解决方案，下一个点位置x(n+1) = x(n) + α(x(n),p(n))p(n)

双向回溯可进一步减少时间

## Adagrad算法
Adagrad优化算法被称为自适应学习率优化算法，帮助算法在梯度大的参数方向减缓学习速率，而在梯度小的参数方向加快学习速率，这就导致了神经网络的训练速度的加快。

## AdaDelta
daDelta算法有两大优势：(1) 像RMSProp算法一样解决了AdaGard算法学习率不断降低的问题，(2)动态确定学习率， 不需要提前设置学习率这一超参数。
